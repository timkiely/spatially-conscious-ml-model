---
title: "Data and Methodology"
output: pdf_document
bibliography: bib/bibliography.bib
header-includes:
  - \usepackage{amsmath}
---

```{r method load package, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))
```

## Methodology Overview

Our goal is to compare "spatially-conscious" machine learning predictive models to traditional feature engineering techniques. To accomplish this comparison, we create three separate modeling data sets: 

- **Base modeling data:** includes building characteristics such as size, taxable value, usage and others
- **Zip Code modeling data:** includes base data as well as aggregations of data at the zip-code level
- **Spatial Lag modeling data:** includes base data as well as aggregations of data within 500-meters of each building

\noindent The second and third modeling data sets are incremental variations of the first, using competing feature engineering techniques to extract additional predictive power from the data. To accomplish this, we combine three open-source data repositories provided by New York City via [nyc.gov](nyc.gov) and [data.cityofnewyork.us](data.cityofnewyork.us). Our base modeling data set includes all building records and associated sales information from 2003-2017. For each of the three modeling data set, we also compare 2 predictive modeling tasks, using a different outcome variable for each: 

1) **Classification task: Probability of Sale** The probability that a given property will sell in a given year
2) **Regression task: Sale Price per square foot** Given that a property sells, how much is the sale price per square foot?

\noindent The six distinct modeling tasks/data combinations are shown in table \ref{tab:modeltable}. 

```{r model table, fig.cap= "All Six Predictive Models", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
tribble(
  ~`#`, ~Model, ~`Model Task`, ~Data, ~`Outcome Var`, ~`Outcome Type`, ~`Eval Metric`
  , 1, "Probability of Sale", "Classification", "Base", "Building Sold", "Binary", "AUC"
  , 2, "Probability of Sale", "Classification", "Zip Code", "Building Sold", "Binary", "AUC"
  , 3, "Probability of Sale", "Classification", "Spatial Lags", "Building Sold", "Binary", "AUC"
  
  , 4, "Sale Price", "Regression", "Base", "Sale Price per SF", "Continuous", "RMSE"
  , 5, "Sale Price", "Regression", "Zip Code", "Sale Price per SF", "Continuous", "RMSE"
  , 6, "Sale Price", "Regression", "Spatial Lag", "Sale Price per SF", "Continuous", "RMSE"
  ) %>% my_kable(caption = "Six Predictive Models", label =  "modeltable")
```

We conduct our analysis in a two-stage process. In Stage 1, we use the Random Forrest algorithm to evaluate the suitability  of the data for our feature engineering assumptions. In Stage 2, we create subsets of the modeling data sets based on analysis conducted in Stage 1. We then compare the performance of different algorithms across all modeling data sets and prediction tasks. The following is an outline of our complete analysis process:\newline

\noindent \textbf{Stage 1: Random Forrest, all data}

1) Create a "base" modeling data set by sourcing and combining building characteristic and sales data from open-source New York City repositories
2) Create a "zip" modeling data set by aggregating the base data at a Zip-Code level and appending these features to the base data
3) Create a "spatial lag" modeling data set by aggregating the base data within 500 meters of each building and appending these features to the base data
4) Train a Random Forrest model on all three data sets, for both classification and regression tasks
5) Evaluate the performance of the various Random Forrest models on hold-out test data
6) Analyze the prediction results by building type and geography, identifying those buildings for which our feature-engineering assumptions (e.g., 500 meter radii spatial lags) are most appropriate\newline


\noindent \textbf{Stage 2: Many algorithms, refined data}

7) Create subsets of the modeling data based on analysis conducted in Stage 1
8) Train machine learning models on the refined modeling data sets using several algorithms, for both classification and regression tasks
9) Evaluate the performance of the various models on hold-out test data
10) Analyze the prediction results of the various algorithm/data/task combinations

## Data

### Data Sources

The New York City government makes available an annual data set which describes all tax lots in the five boroughs. The Primary Land Use and Tax Lot Output data set, known as [PLUTO](https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts[year]=0)[^1], contains a single record for every tax lot in the city along with a number of building and tax-related attributes such as Year Built, Assessed Value, Square Footage, number of stories, and many more. At the time of this writing, NYC has made this data set available for all years between 2002-2017, excluding 2008. For convenience, we also exclude the 2002 data set from our analysis because corresponding sales information is not available for that year. Importantly for our analysis, the latitude and longitude of the tax lots are also made available, allowing us to locate in space each building and to build geospatial features from the data. 

[^1]: https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts[year]=0

Ultimately, we are interested in both the occurrence and the amount of real estate sales transactions. Sales transactions are also made available by the New York City government, known as [NYC Rolling Sales Data](http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page)[^2]. At the time of this writing, sales transactions are available for the years 2003-2017. The sales transactions data contains additional data fields describing time, place, and amount of sale as well as additional building characteristics. Crucially, the sales transaction data does not include geographical coordinates, making it impossible to perform geospatial analysis without first mapping the sales data to PLUTO.

[^2]: http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page

Prior to mapping to PLUTO, the sales data must first be transformed to include the proper mapping key. New York City uses a standard key of Borough-Block-Lot to identify tax lots in the data. For example, 31 West 27th Street is located in Manhattan, on block 829 and lot 16, therefore, its Borough-Block-Lot (BBL) is 1_829_16 (the 1 represents Manhattan). The sales data contains BBL's at the building level, however, the sales transactions data does not appropriately designate condos as their own BBL's. Mapping the sales data directly to the PLUTO data results in a mapping error rate of 23.1%. Therefore, the sales transactions data must first be mapped to another data source, the NYC Property Address Directory, or [PAD](https://data.cityofnewyork.us/City-Government/Property-Address-Directory/bc8t-ecyu/data)[^3], which contains an exhaustive list of all BBL's in NYC. Once the sales data is combined with PAD, the data can be mapped to PLUTO with an error rate of 0.291% (See: Figure \ref{fig:Data Schema}).

[^3]: https://data.cityofnewyork.us/City-Government/Property-Address-Directory/bc8t-ecyu/data


```{r Data Schema, fig.cap="Overview of Data Sources",  out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE, eval = T}
knitr::include_graphics("Sections/tables and figures/Data Schema.png")
```

After the Sales Transactions data has been mapped to PAD, it can then be mapped to PLUTO. The sales data is normalized and filtered so that only BBL's with less than or equal to 1 transactions in a year occur. The final data set is an exhaustive list of all tax lots in NYC for every year between 2003-2017, whether that building was sold, for what amount, and several other additional variables. A description of all variables can be seen in Table \ref{tab:descripTable}.

```{r descripTable,  out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE, eval = T}
options(scipen = 999)
read_csv("tables and figures/desc_table.csv") %>% 
  select(-contains("q")) %>% 
  mutate(nobs = scales::comma(nobs)
         , mean = round(mean, 2)
         , sd = round(sd, 2)
         , min = round(min, 2)
         , max = round(max, 2)
         , median = round(median, 2)) %>% 
  mutate_at(vars(nobs:n_missing), funs(scales::comma)) %>% 
  my_kable(caption = "Description of Base Data", label =  "descripTable")
```


### Global filtering of the data

We will only include building categories of significant interest in the modeling data. Generally speaking, by significant interest we are referring to building types that are regularly bought and sold on the free market. These include residences, office buildings and industrial buildings, and exclude things like government-owned buildings and hospitals. We also exclude hotels as they tend to be comparatively rare in the data and exhibit unique sales characteristics. The included building types are displayed in Table \ref{tab:categoryTable}.

```{r echo=FALSE, fig.cap= "Building Types Included in Modeling Data", message=FALSE, warning=FALSE, paged.print=FALSE}

category_table <- tribble(
  ~Category, ~Description
  , "A", "ONE FAMILY DWELLINGS"
  , "B", "TWO FAMILY DWELLINGS"
  , "C", "WALK UP APARTMENTS"
  , "D", "ELEVATOR APARTMENTS"
  , "F", "FACTORY AND INDUSTRIAL BUILDINGS"
  , "G", "GARAGES AND GASOLINE STATIONS"
  , "L", "LOFT BUILDINGS"
  , "O", "OFFICES"
)


category_table %>% 
  my_kable(caption = "Included Building Cateogory Codes", label = "categoryTable", latex_options = "basic")

```


The data is further filtered to include only records with equal to or less than 2 buildings per tax lot. This effectively excludes large outliers in the data, including multi-building tax lots such as the World Trade Center and Stuyvesant Town. The global filtering of the data set reduces the base modeling data from 12,012,780 records down to 8,247,499, retaining 68.6%% of the original data.

### Exploratory Data Analysis

```{r Load EDA tables, echo=FALSE, message=FALSE, warning=FALSE, eval = T}
by_year <- read_csv("tables and figures/eda_by_year.csv")
by_boro <- read_csv("tables and figures/eda_by_boro.csv")
by_class <- read_csv("tables and figures/eda_by_class.csv")
by_bt_boro_num_sales <- read_csv("tables and figures/eda_by_bt_by_boro_num_sales.csv")
by_bt_boro_med_sales <- read_csv("tables and figures/eda_by_bt_by_boro_median_sales.csv")

```

The data contain building and sale records across the five boroughs of New York City for the years 2003-2017. One challenge with creating a predictive model of real estate sales data is the heterogeneity within the data in terms of frequency of sales and sale price. These two metrics (sale occurrence and amount) vary greatly across year, borough and building classes (among other attributes). Table \ref{tab:by_year} displays statistics which describe the base data set (pre-filtered) by year. Note how the frequency of transactions (# of Sales) and the sale amount (Median Sale $/SF) tend to covary, particularly through the downturn of 2009-2012. This may be due to the fact that the relative size of transactions tends to decrease as capital becomes more constrained. 

```{r by_year, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
by_year %>% my_kable(caption = "Sales By Year", label = "by_year", latex_options = "basic")
```

Similar variance can be seen across asset types. Table \ref{tab:by_class} shows all buildings classes in the 2003-2017 period. Unsurprisingly, residences tend to have the highest volume of sales while offices tend to have the highest sale prices. 

```{r by_class, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
by_class %>% 
  filter(!is.na(`Bldg Code`)) %>% 
  my_kable(caption = "Sales By Asset Class", label = "by_class", latex_options = "basic")
```

Sale price per square foot, in particular, varies greatly across geography and asset class. Table \ref{tab:by_class_boro} shows the breakdown of sales prices by borough and asset class. Manhattan tends to command the highest sale-price-per-square foot across asset types. "Commercial" asset types such as Office and Elevator Apartments tend to fetch much lower price-per-square foot than do residential classes such as one and two-family dwellings. Table \ref{tab:by_class_boro_num} shows the number of transactions across the same dimensions. 


```{r by_class_boro, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

by_bt_boro_med_sales %>% my_kable(caption = "Sale Price Per Square Foot by Asset Class and Borough", label = "by_class_boro")

```

```{r by_class_boro_num, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

by_bt_boro_num_sales %>% 
  mutate_at(vars(BK:SI), funs(scales::comma)) %>% 
  my_kable(caption = "Number of Sales by Asset Class and Borough", label = "by_class_boro_num")

```


## Feature Engineering

### Base Modeling Data

The base modeling data set is constructed by combining several open-source data repositories, outlined in the Data Sources section. In addition to the data provided by New York City, several additional features are engineered and appended. A summary table of the additional features are presented in Table \ref{tab:baseModelDataFeats}. A binary variable is created to indicate whether a tax lot has a building on it (i.e., whether it is an empty plot of land or not). In addition, building types are quantified by what percent of the square footage belongs to the major property types: Commercial, Residential, Office, Retail, Garage, Storage, Factory and Other. 


```{r Table 1, fig.cap= "Base Modeling Features", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}


readr::read_rds("tables and figures/table1.rds") %>% 
  my_kable(caption = "Base Modeling Data Features", label = "baseModelDataFeats")

```



Importantly, two variables are created from the Sales Prices: A price-per-square-foot figure ("Last_Sale_Price") and a total Sale Price ("Last_Sale_Price_Total"). Sale Price per Square foot eventually becomes the outcome variable in one of the predictive models, even though it is referred to as Sale Price. Further features are derived which carry forward the previous sale price of a tax lot, if there is one, through successive years. Previous Sale Price is then used to create Simple Moving Averages (SMA), Exponential Moving Averages (EMA), and percent change measurements between the moving averages. In total, 69 variables are input to the feature engineering process and 92 variables are output. The final base modeling data set is 92 variables by 8,247,499 rows. 


### Zip Code Modeling Data

The first of the two comparative modeling data sets is the Zip Code modeling data. Using the base data as a starting point, several features are generated to describe characteristics of the Zip Code where each tax lot resides. A summary table of the Zip Code level features is presented in \ref{tab:zipcodemodelfeats}. 


```{r Table 2, fig.cap= "Zip Code Modeling Features", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

readr::read_rds("tables and figures/table2.rds") %>% 
  mutate(Feature = gsub("_"," ",Feature)) %>% 
my_kable(caption = "Zip Code Modeling Data Features", label = "zipcodemodelfeats")

```

In general, the base model data features are aggregated to a Zip Code level and attached to the individual observations, including SMA and EMA calculations. Additionally, a second set of features are added, denoted as "bt_only", which filter only for tax lots of the same building type and aggregate to the Zip Code level. In total, the Zip Code feature engineering process inputs 92 variables and outputs 122 variables. 


### Spatial Lag Modeling Data

Spatial lags are variables created from physically proximate observations. For example, calculating the average age of all buildings within 100 meters of a tax lot constitutes a spatial lag. Creating spatial lags presents both advantages and disadvantages in the modeling process. Spatial lags allow for much more fine-tuned measurements of a building's surrounding area. Intuitively, knowing the average sale price of all buildings within 500 meters of a building can be more informative than knowing the sale prices of all buildings in the same Zip Code. However, creating spatial lags is computationally expensive. In addition, it can be difficult to set a proper radius for the spatial lag calculation; in a city, 500 meters may be appropriate (for certain building types), whereas several kilometers or more may be  appropriate for less densely populated areas. In this paper, we present a solution for the computational challenges and suggest a potential approach to solving the radius-choice problem.

#### Creating the Point-Neighbor Relational Graph

To build our spatial lags, for each point in the data, we must identify which of all other points in the data fall within a specified radius. This requires iteratively running point-in-polygon operations, i.e., "given polygon P and an arbitrary point q, determine whether point q is enclosed by the edges of the polygon" [@Huang1996]. This process is conceptually illustrated in figure \ref{fig:Spatial Lag Feature Process}. 

```{r Spatial Lag Feataure Process, fig.cap="Spatial Lag Feature Creation Process",  out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE, eval = T}
knitr::include_graphics("Sections/tables and figures/Spatial Lag Creation.png")
```

Given that, for every point $q_i$ in our data set, we need to determine whether every other point $q_i$ falls within a given radius, this means that the time-complexity of our operation can be approximated as:


$$
O(N(N-1))
$$

Since the number of operations approaches $N^2$, calculating spatial lags for all 8,247,499 observations in our modeling data would be unfeasible from a time and computation perspective. Assuming that tax lots rarely if ever move over time, we first reduced the task to the number of unique tax lots in New York City from 2003-2017, which is 514,124 points. Next, we implemented an indexing technique that greatly speeds up the process of creating a point-neighbor relational graph. The indexing technique both reduces the relative search space for each computation and also allows for parallelization of the point-in-polygon operations by dividing the data into a gridded space. The gridded spatial indexing process is outlined in Algorithm \ref{alg:spatial1}.

\begin{algorithm}
  \caption{Gridded Spatial Indexing}
  \label{alg:spatial1}
  \begin{algorithmic}[1]
      \For{\texttt{each grid partition $G$}}
        \State \texttt{Extract all points points $G_i$ contained within partition $G$}
        \State \texttt{Calculate convex hull $H(G)$ such that the buffer extends to distance $d$}
        \State \texttt{Define Search space $S$ as all points within Convex hull $H(G)$}
        \State \texttt{Extract all points $S_i$ contained within $S$}
          \For{\texttt{each data point $G_i$}}
            \State \texttt{Identify all points points in $S_i$ that fall within $abs(G_i+d)$}
        \EndFor
      \EndFor
  \end{algorithmic}
\end{algorithm}


\noindent Each gridded partition of the data is married with a corresponding search space $S$, which is the convex hull of the partition space buffered by the maximum distance $d$. In our case, we are buffering the search space by 500 meters. Choosing an appropriate radius for buffering presents an additional challenge in creating spatially-conscious machine learning predictive models. In this paper, we choose an arbitrary radius, and use a two-stage modeling process to test the appropriateness of that assumption. In future work, implementing an "adaptive bandwidth" technique using cross-validation to determine the optimal radius could be done. 

By partitioning the data into spatial grids, we are able to reduce the search-space for each operation by an arbitrary number of partitions $G$. This improves the base run-time complexity to:

$$
O(N(\frac{N-1}{G})
$$

\noindent By making G arbitrarily large (bounded by computational resources only), we reduced runtime substantially. Furthermore, binning the operations into grids allowed us to parallelize the computation, further reducing the overall run time. Figure \ref{fig:Spatial Indexing Process} shows a comparison of computation times between the basic point-in-polygon technique and a sequential version of the grided indexing technique. Note that the grid method starts out as slower than the basic point-in-polygon technique due to pre-processing overhead, but wins out in terms of speed as complexity increases. This graph also does not reflect parallelization of the operation, which further reduces the time required to calculate the point-neighbor relational graph. 

```{r Spatial Indexing Process, fig.cap="Spatial Index Time Comparison",  out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE, eval = T}
knitr::include_graphics("Sections/tables and figures/Example Spatial Indexing Techniques.png")
```

#### Calculating Spatial Lags

Once the origin-neighbor relational graph has been constructed, we can aggregate the data into spatial lag variables. One advantage of using spatial lags is the rich number of potential features which can be engineered. Spatial lags can be weighted based on a distance function, e.g., physically closer observations can be given more weight. For our modeling purposes, we created two sets of features: distance weighted features (denoted with a "_dist" in Table \ref{tab:SpLAgFeats}) and simple average features (denoted with "_basic" in Table \ref{tab:SpLAgFeats}). SMA and EMA as well as percent changes were also calculated. 

Temporal and spatial derivatives of the Spatial Lag features presented in Table \ref{tab:SpLAgFeats} were also added to the model, including: variables weighted by euclidean distance ("dist"), basic averages of the spatial lag radius ("basic mean"), Simple Moving Averages ("SMA") for 2 years, 3 years and 5 years, exponential moving averages ("EMA") for 2 years, 3 years and 5 years, and year-over-year percent changes for all variables ("perc change"). In total, the spatial lag feature engineering process input 92 variables and output 194 variables. A summary of the Spatial Lag features are presented in Table \ref{tab:SpLAgFeats}.

```{r SpLagFeats, fig.cap= "Spatial Lag Modeling Features", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width = '100%'}

readr::read_rds("tables and figures/table3.rds") %>%
  mutate_at(vars(Min:Max), function(x) ifelse(is.infinite(x),NA,x)) %>% 
  my_kable(caption = "All Spatial Lag Features", label = "SpLAgFeats")

```


## Outcome Variables

The final step in creating the modeling data is to define the outcome variables. The outcome variables reflect the prediction tasks; a binary variable for classification and a continuous variable for regression:

1) **Binary: Sold** whether a tax lot sold in a given year. Used in the Probability of Sale classification model.
2) **Continuous: Sale Price per SF** The price-per-square foot associated with a transaction, if a sale took place. Used in the Sale Price Regression model. 

\noindent Table \ref{tab:OutcomeDistro} describes the distributions of both outcome variables.


```{r table 4, fig.cap= "Distributions for Outcome Variables", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

  merge(readr::read_rds("tables and figures/sold_summary_table5.rds")
        ,readr::read_rds("tables and figures/sale_price_summary_table4.rds")
        , sort = FALSE) %>% 
  my_kable(caption = "Distributions for Outcome Variables", label = "OutcomeDistro", latex_options = "basic")

```


## Algorithms Comparison

We implemented and compared several algorithms across our two-stage process. In Stage 1, the Random Forrest algorithm was used to identify the optimal subset of building types and geographies for our spatial lag aggregation assumptions. In Stage 2, we analyze the performance of the Random Forrest algorithm again, as well as Generalized Linear Model (GLM), Gradient Boosting Machines (GBM), and a Feed-Forward Artificial Neural Network (ANN). Each algorithm is appropriate for both classification and regression tasks. 

### Random Forest

The Random Forest concept was proposed by Leo Breiman in 2001 as the ensemble of prediction decision trees iteratively trained across randomly generated subsets of data [@breiman2001random]. Algorithm \ref{alg:RandomForestAlo} outlines the procedure [@hastie01statisticallearning]. 


\begin{algorithm}
  \caption{Random Forest for Regression or Classification}\label{alg:RandomForestAlo}

\begin{enumerate}
  \item For $b = 1$ to $B$
  \begin{enumerate}
    \item Draw a bootstrap sample $Z$ of the size $N$ from the training data.
    \item Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached.
  \begin{enumerate}
    \item Select $m$ variables at random from the $p$ variables
    \item Pick the best variable/split-point among the $m$.
    \item Split the node into two daughter nodes.
  \end{enumerate}
  \end{enumerate}
  \item Output the ensemble of trees $\{T_b\}_1^B$.
\end{enumerate}

To make a prediction at a new point $x$:

\textit{Regression:} $\hat{f}_{rf}^B(x) = \frac{1}{B}\sum_{b=1}^{B}T_b(x)$

\textit{Classification:} Let $\hat{C_b}(x)$ be the class prediction of the $b$th random-forest tree. Then $\hat{C_{rf}^B}(x)=$ \textit{majority vote} $\{\hat{C}_{b}(x)\}_1^B $

\end{algorithm}

Previous works (see: @antipov12; also @Schernthanner2016) have found the Random Forest algorithm [@Breiman2001] suitable to prediction tasks involving real estate. While algorithms exist that may marginally outperform Random Forest in terms of predictive accuracy (such as neural networks and functional gradient descent algorithms), Random Forest is highly scalable and parallelizable, and therefore a natural choice for quickly assessing the predictive power of different feature engineering techniques. For these reasons and more outlined below, we selected Random Forrest as the primary algorithm for Stage 1 of our modeling process.

Random Forest can be used for both classification and regression tasks. The Random Forest algorithm works by generating a large number of independent classification or regression decision trees and then employing  majority voting (for classification) or averaging (for regression) to generate predictions. Over a data set of N rows by M predictors, a bootstrap sample of the data is chosen (n < N) as well as a subset of the predictors (m < M). Individual decision/regression trees are built on the n by m sample. Because the trees can be built independently (and not sequentially, as is the case with most functional gradient descent algorithms), the tree building process can be executed in parallel. With a sufficiently large number of cores, the model training time can be significantly reduced.

We chose Random Forrest as the algorithm for Stage 1 because: 

1) The algorithm can be parallelized and is relatively fast compared to neural networks and functional gradient descent algorithms 
2) Can accommodate categorical variables with many levels. Real estate data often contains information describing the location of the property, or the property itself, as one of a large set of possible choices, such as neighborhood, county, census tract, district, property type, and zoning information. Because factors need to be recoded as individual dummy variables in the model building process, factors with many levels will quickly encounter the curse of dimensionality in multiple regression techniques.
3) Appropriately handles missing data. Predictions can be made with the parts of the tree which are successfully built, and therefore, there is no need to filter out incomplete observations or impute missing values. Since much real estate data is self reported, incomplete fields are common in the data.
4) Robust against outliers. Because of bootstrap sampling, outliers appear in individual trees less often, and therefore, are reduced in terms of importance. Real estate data, especially with regards to pricing, tends to contain outliers. For example, the dependent variable in one of our models, Sale Price, shows a clear divergence in median and mean, as well as a maximum significantly higher than the third quartile.
5) Can recognize non-linear relationships in data, which is useful when modeling spatial relationships. 
6) Is not affected by co-linearity in the data. This is highly valuable as real estate data can be highly correlated. 

To run the model, we have chosen the h2o.randomForest implementation from the h2o R open source library. The h2o implementation of the Random Forest algorithm is particularly well-suited for high parallelization. For more information, see: [https://www.h2o.ai/](https://www.h2o.ai/).


### Generalized Linear Model

### Gradient Boosting Machine

### Feed-Forward Artificial Neural Netwrok



## Model Validation

The goal of the predictive models are to be able to successfully predict both the probability and amount of real estate sales into the near future. As such, our models will use out-of-time validation to assess performance. As shown in Figure  \ref{fig:Train Test Validate} The models will be trained using data from 2003-2015. 2016 modeling data will be used during the model training process as cross-validation data. Finally, we will score our model using 2017 as a hold-out sample. Using out-of-time validation should ensure that the models generalize well into the immediate future. 


```{r Train Test Validate, fig.cap="Out-of-time validation",  out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE, eval = T}
knitr::include_graphics("Sections/tables and figures/Train Validate Test.png")
```

## Variable Selection

For ease of processing and to improve the ability of the model to generalize into the future, a variable selection step is added to the modeling process. A Random Forest model is first trained on a 1% sub-sample of the modeling data. Variable importance of the resulting model is calculated using the technique proposed by @Friedman2001, i.e., for a collection of decision trees $[T_m]_{1}^{m}$: 


\[
  \hat{I_{j}^{2}} = \frac{1}{M} \sum_{m=1}^{M}\hat{I_{j}^{2}}(T_m)
\]

Where influence $I$ for variable $j$ is calculated as the sum of corresponding improvements in squared-error for node tree $T$. After calculating variable importance for the model data subset, the variables are rank-ordered by descending importance. Variables which account for 80% of the total variable importance are chosen to advance to the model training round on the full modeling data sets. 


## Evaluation Metrics

We have chosen evaluation metrics that will allow us to easily compare the performance of the models against other models with the same outcome variable. The classification models (Probability of Sale) will be compared using Area Under the ROC Curve (AUC). The regression models (Sale Price) will be compared using Root Mean Squared Error (RMSE). Both evaluation metrics are common for their respective outcome variable types, and as such will be useful for comparing within model-groups. 


### Area Under ROC Curve (AUC)

A classification model typically outputs a probability that a given row in the data belongs to a group. In the case of binary classification, the value falls between 0 and 1. There are many techniques for determining the cut off threshold for classification; a typical method is to assign anything above a 0.5 into the "1" or positive class. An ROC curve (receiver operating characteristic curve) plots the True Positive Rate vs. the False Positive rate at different classification thresholds; it is a measurement of the performance of a classification model across all possible thresholds, and therefore sidesteps the need to arbitrarily assign a cutoff. 

Area Under the ROC Curve, or AUC measures the entire two-dimensional area underneath the ROC curve. It is the integration of the curve from (0,0) to (1,1), defined as $AUC = \int_{(0,0)}^{(1,1)} f(x)dx$. 

AUC provides a relatively standard measure of performance across all possible classification thresholds, and can be interpreted as the probability that the model ranks a random positive example more highly than a random negative example. A value of 0.5 represents a perfectly random model, while a value of 1.0 represents a model that can perfectly discriminate between the two classes. AUC is useful for comparing classification models against one another because they are both scale and threshold-invariant.

One of the drawbacks to AUC is that is does not describe the trade-offs between false positives and false negatives. In certain circumstances, a false positive might be considerably less desirable than a false negative, or vice-versa. For our purposes, we rank false positives and false negatives as equally undesirable outcomes.

### Root Mean Squared Error

The Root Mean Squared Error (RMSE) is a common measurement of the differences between values predicted by a regression model and the observed values. It is formally defined as $RMSE = \sqrt{ \frac{\sum_{1}^{T} (\hat{y}_t - y_t)^2}{T} }$, where $\hat{y}$ represents the prediction and $y$ represents the observed value at observation $t$. 

Lower RMSE scores are typically more desirable. An RMSE value of 0 would indicate a perfect fit to the data. RMSE can be difficult to interpret on its own, however, it is useful for comparing models with similar outcome variables. In our case, the outcome variables (Sales Price per Square Foot) are consistent across modeling data sets, and therefore can be reasonably compared using RMSE. 


