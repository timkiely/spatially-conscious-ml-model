\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\setlength{\parindent}{2em}
\setlength{\parskip}{0em}
\usepackage{placeins}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{chngcntr}
\usepackage{microtype}
\usepackage{ragged2e}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\onehalfspacing
\counterwithin{figure}{section}
\counterwithin{table}{section}
\linespread{2}

\begin{document}

\pagenumbering{gobble}

\titlepage
\center
\vspace{4 cm}
\LARGE

\bf 
The Spatially-Conscious Machine Learning Model  

\Large

Leveraging Spatial Analysis to Boost the Accuracy of Real Estate Sales
Predictions

\rm
\normalsize

By

\textbf{Tim Kiely}

Thesis Project

Submitted in partial fulfillment of the

requirements for the degree of

~

MASTER OF SCIENCE IN DATA SCIENCE

~

Northwestern University

~

Nathaniel D. Bastian, Ph.D., First Reader

Candice Bradley, Second Reader

\newpage
\normalsize
\singlespace
\begin{abstract}

Successfully predicting gentrification could have many social and commercial applications, however, real estate sales are difficult to predict because they belong to a chaotic system comprised of intrinsic and extrinsic characteristics, perceived value, and market speculation. We combine modern techniques of data science and machine learning with traditional spatial analysis to create robust real estate prediction models for both classification and regression tasks. We compare several cutting edge machine learning algorithms across spatial, semi-spatial and non-spatial feature engineering techniques, and we empirically show that spatially-conscious machine learning models outperform non-spatial models when married with advanced prediction techniques such as feed-forward artificial neural networks and gradient boosting machine models. 

\textbf{Keywords:} Real estate, Artificial neural networks, Machine learning, Recommender systems, Supervised learning, Predictive modeling

\end{abstract}

\newpage
\normalsize
\singlespace
\tableofcontents
\doublespace
\newpage
\pagenumbering{arabic}

\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \fancyhead[R] \thepage
  \setlength\footskip{0pt}
}
\pagestyle{plain}
\justify

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{the-spatially-conscious-machine-learning-model}{%
\subsection{The Spatially-Conscious Machine Learning
Model}\label{the-spatially-conscious-machine-learning-model}}

Things near each other tend to be like each other. This concept is a
well-known problem in traditional spatial analysis and is typically
referred to as \emph{spatial autocorrelation}. In statistics, this is
said to ``reduce the amount of information'' pertaining to spatially
proximate observations as they can, in part, be used to predict each
other. But can spatial features be used in a machine-learning context to
make better predictions? This work demonstrates that the addition of
``spatial lag'' features to machine learning models significantly
increases accuracy when predicting real estate sales and sale prices.

\hypertarget{motivation-combating-income-inequality-by-predicting-gentrification}{%
\subsection{Motivation: Combating Income Inequality by Predicting
Gentrification}\label{motivation-combating-income-inequality-by-predicting-gentrification}}

Researchers at the Urban Institute (Solomon Greene and Lei 2016)
recently identified ``Economic Exclusion'' as a powerful contributor to
income inequality in the United States. ``Economic Exclusion'' can be
defined as follows: vulnerable populations--disproportionately
communities of color, immigrants, refugees, and women--who are
physically displaced by local economic prosperity enter a continuous
cycle of diminished access to good jobs, good schools, health care
facilities, public spaces, and other economic and social resources.
Diminished access leads to more poverty, which leads to more
displacement. This self-reinforcing cycle of poverty and exclusion
gradually exacerbates income inequality over the course years and
generations. What can be done to intervene?

Stopping Economic Exclusion requires preventing displacement. As defined
by Clay (1979), ``displacement'' can be thought of as the negative
consequence of gentrification, where affordability pressures force
vulnerable populations to relocate due to localized economic prosperity.
Reliably predicting gentrification would be a valuable tool for
preventing displacement at an early stage; however, such a task has
proven difficult historically.

When an area experiences economic growth, increased housing demands and
subsequent affordability pressures can lead to voluntary or involuntary
relocation of low-income families and small businesses. Government
agencies and nonprofits tend to intervene once displacement is already
underway, and after-the-fact interventions can be costly and
ineffective. As explained by Solomon Greene and Lei (2016), several
preemptive actions exist which can be deployed to stem divestment and
ensure that existing residents benefit from local prosperity. Potential
interventions include regulation, affordable housing, micro-financing,
tax subsidies, vouchers, and more. Not unlike medical treatment, early
detection is the key to success.

Consequently, in 2016, the Urban Institute published a series of essays
outlining the few ways city governments employ ``Big data and
crowdsourced data'' to ``better identify excluded people and
communities, connect them to needed services, and improve service
delivery'', noting that ``much more could be done'' (Solomon Greene and
Lei 2016). This work seeks to further the research into using free, open
data, open-source software and cutting-edge techniques to predict trends
in gentrification, with the aim of identifying high-risk households and
areas in need of intervention.

Many government agencies have, to date, demonstrated the benefits of
applied predictive modeling, ranging from prescription drug abuse
prevention to homelessness intervention to recidivism reduction (Ritter
2013). However, few, if any, examples exist of large-scale, systematic
applications of data analysis to aid vulnerable populations experiencing
displacement. This paper belongs to an emerging trend known as the
``science of cities'' which aims to use large data sets and advanced
simulation and modeling techniques to understand and improve urban
patterns and how cities function (Batty 2013).

This work explores techniques that can dramatically boost the accuracy
of existing gentrification prediction models. We use real estate
transactions in New York City, both their occurrence (probability of
sale) and their dollar amount (sale price per square foot) as a proxy
for gentrification. The technique marries the use of machine-learning
predictive modeling with ``spatial-lag'' features typically seen in
geographically-weighted regressions (GWR). We employ a two-step modeling
process in which we 1) determine the optimal building types and
geographies suited to our feature engineering assumptions and 2) perform
a comparative analysis across several state-of-the-art algorithms
(Generalized Linear Model, Random Forest, Gradient Boosting Machine,
Artificial Neural Network). We conclude that spatially-conscious machine
learning models consistently outperform traditional real estate
valuation and predictive modeling techniques.

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

The literature review for this paper discusses the academic study of
Economic Displacement, primarily as it relates to gentrification. We
also examine ``mass appraisal techniques,'' which are automated
analytical techniques used for valuing large numbers of real estate
properties. Finally, we examine recent applications of machine learning
as it relates to predicting gentrification.

\hypertarget{what-is-economic-displacement}{%
\subsection{What is Economic
Displacement?}\label{what-is-economic-displacement}}

Economic Displacement has been intertwined with the study of
gentrification since shortly after the latter became academically
relevant in the 1960s. The term ``gentrification'' was first used by
Ruth Glass in 1964 to describe the ``gentry'' in low-income
neighborhoods in London. Originally, academics thought of gentrification
as a ``tool of revitalization for declining neighborhoods'' (Zuk 2015),
however, in 1979 Phillip Clay highlighted that gentrification brought
both positive and negative consequences to local populations and
described the negative as Economic Exclusion (Clay 1979). Today, the
term has adopted a more neutral connotation, describing the ``spatial
organization and re-organization of human dwelling and activity'' (Zuk
2015). Specific to cities, recent literature defines gentrification as
``the transformation of a working-class or vacant area of the central
city into middle-class residential or commercial use'' (Lees 2008).

Studies of gentrification and displacement generally take two approaches
in the literature: supply-side and demand-side, or ``the flows of
capital versus flows of people to neighborhoods,'' respectively (Zuk
2015). Supply-side arguments for gentrification tend to focus on
``private capital investment, public policies, and public investments''
(Zuk 2015) and are much more often the subject of academic literature on
Economic Displacement. This kind of research may be more common because
it has the advantage of being more directly linked to influencing public
policy (as opposed to controlling the flows of people). According to
Dreier (2004), public policies that can increase Economic Displacement
have been, among others, automobile-oriented transportation
infrastructure spending and mortgage interest tax deductions for
homeowners. Others that have argued for supply-side gentrification
include Smith (1979), who stated that the return of capital from the
suburbs to the city, or the ``political economy of capital flows into
urban areas'' are what primarily drive both the positive and negative
consequences of urban gentrification.

More recently, researchers have explored Economic Displacement as a
contributor to income inequality (Reardon 2011); (Watson 2009). For
example, Reardon (2011) argues that concentrations of wealth allow
``certain households to sort themselves according to their preferences
-- and control local political processes that continue exclusion.'' The
exercising of political influence by prosperous residents results in a
feedback loop producing downward economic pressure on households who
lack political influence. Gentrification prediction tools could be used
to help break such feedback loops through early identification and
intervention.

Many studies conclude that gentrification in most forms leads to
exclusionary economic displacement; however, Zuk (2015) characterizes
the results of many recent studies as ``mixed, due in part to
methodological shortcomings.'' In this paper, we attempt to further the
understanding of gentrification prediction by demonstrating a technique
to better predict real estate sales in New York City.

\hypertarget{a-review-of-mass-appraisal-techniques}{%
\subsection{A Review of Mass Appraisal
Techniques}\label{a-review-of-mass-appraisal-techniques}}

Much research on predicting real estate prices has been in service of
creating mass appraisal models. Local governments most commonly use mass
appraisal models to assign taxable values to properties. Mass appraisal
models share many characteristics with predictive machine learning
models, in that they are data-driven, standardized methods that employ
statistical testing (Eckert 1990). A variation on mass appraisal models
are the ``automated valuation models'' (AVM), which use ``often the same
methodological framework of mass appraisal\ldots{} a statistical model
and a large amount of property data to estimate the market value of an
individual property or portfolio of properties'' (d'Amato 2017).

Scientific mass appraisal models date back to 1936 with the reappraisal
of St.~Paul, Minnesota (Silverherz 1936). Since that time, and
accelerated with the advent of computers, much statistical research has
been done relating property values and rent prices to various
characteristics of those properties, including characteristics of their
surrounding area. Multiple regression analysis (MRA) has been the most
common set of statistical tools used in mass appraisal, including
Maximum Likelihood, Weighted Least Squares, and the most popular,
Ordinary Least Squares (OLS) (d'Amato 2017). The primary drawbacks of
MRA techniques are ``excessive multicollinearity among attributes'' and
``spatial autocorrelation among residuals'' (d'Amato 2017). Another
group of models that seek to correct for spatial dependence are known as
Spatial Auto-Regressive models (SAR), chief among them the Spatial Lag
Model, which aggregates weighted summaries of nearby properties to
create independent regression variables (d'Amato 2017).

So-called ``Hedonic'' regression models seek to decompose the price of a
good based on the intrinsic and extrinsic components. Koschinsky (2012)
is a recent and thorough discussion of parametric hedonic regression
techniques. Koschinsky derives some of the variables included in his
models from nearby properties, similar to the technique used in this
paper, and he found these spatial variables to be predictive. The real
estate hedonic model as defined by Koschinsky describes the price of a
property as:

\[
\begin{aligned}
 P_i = P(S_i, N_i, L_i)
\end{aligned}
\]

\noindent where \(P_i\) represents the price of house \(i\), which is a
composite good comprised of a vector of structural characteristics
\(S\), a vector of social and neighborhood characteristics \(N\), and a
vector of locational characteristics \(L\). Specifically, the model
calculates spatial lags on properties of interest using neighboring
properties within 1,000 feet of a sale. The derived variables include
characteristics like average age, the number of poor condition homes,
percent of homes with electric heating, construction grades, and more,
within 1,000 feet of the property in question. Koschinsky found that in
all cases, ``the relation between a home's price and the average price
of its neighboring homes is characterized by positive spatial
autocorrelation'' meaning that homes near each other were typically
similar to each other and priced accordingly. Koschinsky concluded that
locational characteristics should be valued at least as much ``if not
more'' than intrinsic structural characteristics.

As recently as 2015, much research has dealt with mitigating the
drawbacks of MRA, including the use of multi-level hierarchical models.
Fotheringham (2015) explored the combination of Geographically Weighted
Regression (GWR) with time-series forecasting to predict home prices
over time. GWR is a variation on OLS that allows for ``adaptive
bandwidths'' of local data to be included, i.e., for each estimate, the
number of data points included varies and can be optimized using
cross-validation.

\hypertarget{prediction-of-gentrification-using-machine-learning}{%
\subsection{Prediction of Gentrification Using Machine
Learning}\label{prediction-of-gentrification-using-machine-learning}}

Both Mass Appraisal techniques and Automated Valuation Modeling seek to
predict real estate prices using data and statistical methods; however,
traditional techniques typically fall short. These techniques fail
partly because property valuation is inherently a ``chaotic'' process
that ``does not lend itself to binary or linear analysis'' (Zuk 2015).
The value of any given property is a complex combination of fungible
intrinsic characteristics, perceived value, and speculation. The value
of any building or plot of land belongs to a rich network where
decisions about and perceptions of neighboring properties influence the
final market value. Guan et al. (2014) compared traditional MRA
techniques to alternative ``data mining techniques'' resulting in
``mixed results.'' However, as Helbich (2013) states, hedonic pricing
models ``can be improved in two ways: (a) Through novel estimation
techniques, and (b) by ancillary structural, locational, and
neighborhood variables on the basis of Geographic Information System.''
Recent research generally falls into these two buckets: better
algorithms and better data.

In the ``better data'' category, researchers have been striving to
introduce new independent variables to increase the accuracy of
predictive models. Dietzell (2014) successfully used internet search
query data provided by Google Trends to serve as a sentiment indicator
and improve commercial real estate forecasting models. Pivo and Fisher
(2011) examined the effects of walkability on property values and
investment returns. Pivo found that on a 100-point scale, a 10-point
increase in walkability increased property investment values by up to
9\%.

Research into better prediction algorithms does not necessarily happen
at the exclusion of ``better data.'' For example, Fu (2014) created a
prediction algorithm, called ``ClusRanking,'' for real estate in
Beijing, China. ClusRanking first estimates neighborhood characteristics
using taxi cab traffic vector data, inlcuding relative access to
``business areas.'' Then, the algorithm performs a rank-ordered
prediction of investment returns segmented into five categories. Similar
to Koschinsky (2012), though less formally stated, Fu (2014) thought of
a property's value as a composite of individual, peer and zone
characteristics. In their predictive modeling, Fu included
characteristics of the neighborhood (individual), the values of nearby
properties (peer), and the prosperity of the affiliated latent business
area (zone) based on taxi cab data (Fu 2014).

Several other recent studies compare various ``advanced'' statistical
techniques and algorithms either to other advanced techniques or to
traditional ones. Most studies conclude that the advanced,
non-parametric techniques outperform traditional parametric techniques,
while several conclude that the Random Forest algorithm is particularly
well-suited to predicting real estate values.

Kontrimasa (2011) compares the accuracy of linear regression against the
SVM technique and found the latter to outperform. Schernthanner H.
(2016) compared traditional linear regression techniques to several
techniques such as kriging (stochastic interpolation) and Random Forest.
They concluded that the more advanced techniques, particularly Random
Forest, are sound and more accurate when compared to traditional
statistical methods. Antipov and Pokryshevskaya (2012) came to a similar
conclusion about the superiority of Random Forest for real estate
valuation after comparing 10 algorithms: multiple regression, CHAID,
Exhaustive CHAID, CART, 2 types of k-Nearest Neighbors, Multilayer
Perceptron artificial neural network, Radial Basis Function neural
network, Boosted Trees and finally Random Forest.

Guan et al. (2014) compared three different approaches to defining
spatial neighbors: a simple radius technique, a k-nearest neighbors
technique using only distance and a k-nearest neighbors technique using
all attributes. Interestingly, the location-only KNN models performed
best, although by a slight margin. Park (2015) developed several
housing-price prediction models based on machine learning algorithms
including C4.5, RIPPER, Naive Bayesian, and AdaBoost, finding that the
RIPPER algorithm consistently outperformed the other models. Rafiei
(2016) employed a restricted Boltzmann machine (neural network with back
propagation) to predict the sale price of residential condos in Tehran,
Iran, using a non-mating genetic algorithm for dimensionality reduction
with a focus on computational efficiency. The paper concludes that two
primary strategies help in this regard: weighting property sales by
temporal proximity (i.e., sales which happened closer in time are more
alike), and also using a learner to accelerate the recognition of
important features. The paper compares this technique to several other
common neural network approaches and finds that while not necessarily
the only way to get the best answer, it is an efficient and fast way to
get to the best answer.

Finally, we note that many studies, whether exploring advanced
techniques, new data, or both, rely on aggregation of data by some
arbitrary boundary. For example, Turner and Snow (2001) predicted
gentrification in the Washington, D.C. metro area by ranking census
tracts in terms of development. Chapple (2009) created a gentrification
``early warning system'' by identifying low-income census tracts in
central city locations. Barry Bluestone \& Chase Billingham (2010)
analyzed 42 census block groups near rail stations in 12 metro areas
across the United States, studying changes between 1990 and 2000 for
neighborhood socioeconomic and housing characteristics. All of these
studies, and many more, relied on the aggregation of data at the
census-tract or census-block level. In contrast, this paper compares
boundary-aggregation techniques (specifically, aggregating by Zip Codes)
to spatial lag techniques and finds the spatial lag techniques to
generally outperform.

\hypertarget{data-and-methodology}{%
\section{Data and Methodology}\label{data-and-methodology}}

\hypertarget{methodology-overview}{%
\subsection{Methodology Overview}\label{methodology-overview}}

Our goal was to compare ``spatially-conscious'' machine learning
predictive models to traditional feature engineering techniques. To
accomplish this comparison, we created three separate modeling datasets:

\begin{itemize}
\tightlist
\item
  \textbf{Base modeling data:} includes building characteristics such as
  size, taxable value, usage, and others
\item
  \textbf{Zip Code modeling data:} includes the base data as well as
  aggregations of data at the zip-code level
\item
  \textbf{Spatial Lag modeling data:} includes the base data as well as
  aggregations of data within 500-meters of each building
\end{itemize}

\noindent The second and third modeling datasets are incremental
variations of the first, using competing feature engineering techniques
to extract additional predictive power from the data. We combined three
open-source data repositories provided by New York City via
\url{nyc.gov} and \url{data.cityofnewyork.us}. Our base modeling dataset
included all building records and associated sales information from
2003-2017. For each of the three modeling datasets, we also compared two
predictive modeling tasks, using a different dependent variable for
each:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Classification task: Probability of Sale} The probability that
  a given property will sell in a given year (0,1)
\item
  \textbf{Regression task: Sale Price per square foot} Given that a
  property sells, how much is the sale price per square foot? (\$/SF)
\end{enumerate}

\noindent Table \ref{tab:modeltable} shows the six distinct modeling
task/data combinations.

\begin{table}

\caption{\label{tab:model table}\label{tab:modeltable} Six Predictive Models}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{rllllll}
\toprule
\# & Model & Model Task & Data & Outcome Var & Outcome Type & Eval Metric\\
\midrule
1 & Probability of Sale & Classification & Base & Building Sold & Binary & AUC\\
2 & Probability of Sale & Classification & Zip Code & Building Sold & Binary & AUC\\
3 & Probability of Sale & Classification & Spatial Lags & Building Sold & Binary & AUC\\
4 & Sale Price & Regression & Base & Sale Price per SF & Continuous & RMSE\\
5 & Sale Price & Regression & Zip Code & Sale Price per SF & Continuous & RMSE\\
6 & Sale Price & Regression & Spatial Lag & Sale Price per SF & Continuous & RMSE\\
\bottomrule
\end{tabular}}
\end{table}

We conducted our analysis in a two-stage process. In Stage 1, we used
the Random Forest algorithm to evaluate the suitability of the data for
our feature engineering assumptions. In Stage 2, we created subsets of
the modeling data based on the analysis conducted in Stage 1. We then
compared the performance of different algorithms across all modeling
datasets and prediction tasks. The following is an outline of our
complete analysis process:\newline

\noindent \textbf{Stage 1: Random Forest algorithm using all data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Create a ``base'' modeling dataset by sourcing and combining building
  characteristic and sales data from open-source New York City
  repositories
\item
  Create a ``zip'' modeling dataset by aggregating the base data at a
  Zip-Code level and appending these features to the base data
\item
  Create a ``spatial lag'' modeling dataset by aggregating the base data
  within 500 meters of each building and appending these features to the
  base data
\item
  Train a Random Forest model on all three datasets, for both
  classification (probability of sale) and regression (sale price) tasks
\item
  Evaluate the performance of the various Random Forest models on
  hold-out test data
\item
  Analyze the prediction results by building type and geography,
  identifying those buildings for which our feature-engineering
  assumptions (e.g., 500-meter radii spatial lags) are most
  appropriate\newline
\end{enumerate}

\noindent \textbf{Stage 2: Many algorithms using refined data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  Create subsets of the modeling data based on analysis conducted in
  Stage 1
\item
  Train machine learning models on the refined modeling datasets using
  several algorithms, for both classification and regression tasks
\item
  Evaluate the performance of the various models on hold-out test data
\item
  Analyze the prediction results of the various algorithm/data/task
  combinations
\end{enumerate}

\hypertarget{data}{%
\subsection{Data}\label{data}}

\hypertarget{data-sources}{%
\subsubsection{Data Sources}\label{data-sources}}

The New York City government makes available an annual dataset which
describes all tax lots in the five boroughs. The Primary Land Use and
Tax Lot Output dataset, known as
\href{https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts\%5Byear\%5D=0}{PLUTO}\footnote{\url{https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts\%5Byear\%5D=0}},
contains a single record for every tax lot in the city along with a
number of building-related and tax-related attributes such as Year
Built, Assessed Value, Square Footage, number of stories, and many more.
At the time of this writing, NYC had made this dataset available for all
years between 2002-2017, excluding 2008. For convenience, we also
exclude the 2002 dataset from our analysis because corresponding sales
information is not available for that year. Importantly for our
analysis, the latitude and longitude of the tax lots are also made
available, allowing us to locate in space each building and to build
geospatial features from the data.

Ultimately, we were interested in both the occurrence and the amount of
real estate sales transactions. Sales transactions are made available
separately by the New York City government, known as the
\href{http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page}{NYC
Rolling Sales Data}\footnote{\url{http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page}}.
At the time of this writing, sales transactions were available for the
years 2003-2017. The sales transactions data contains additional data
fields describing time, place, and amount of sale as well as additional
building characteristics. Crucially, the sales transaction data does not
include geographical coordinates, making it impossible to perform
geospatial analysis without first mapping the sales data to PLUTO.

Prior to mapping to PLUTO, we first had to transform the sales data to
include the proper mapping key. New York City uses a standard key of
Borough-Block-Lot to identify tax lots in the data. For example, 31 West
27th Street is located in Manhattan, on block 829 and lot 16, therefore,
its Borough-Block-Lot (BBL) is 1\_829\_16 (the 1 represents Manhattan).
The sales data contains BBL's at the building level, however, the sales
transactions data does not appropriately designate condos as their own
BBL's. Mapping the sales data directly to the PLUTO data results in a
mapping error rate of 23.1\% (mainly due to condos). Therefore, the
sales transactions data must first be mapped to another data source, the
NYC Property Address Directory, or
\href{https://data.cityofnewyork.us/City-Government/Property-Address-Directory/bc8t-ecyu/data}{PAD}\footnote{\url{https://data.cityofnewyork.us/City-Government/Property-Address-Directory/bc8t-ecyu/data}},
which contains an exhaustive list of all BBL's in NYC. After combining
the sales data with PAD, the data can then be mapped to PLUTO with an
error rate of 0.291\% (See: Figure \ref{fig:Data Schema}).

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/Data Schema} \caption{Overview of Data Sources}\label{fig:Data Schema}
\end{figure}

After combining the Sales Transactions data with PAD and PLUTO, the
resulting data were filtered so that only BBL's with less than or equal
to 1 transactions in a year occur. The final dataset is an exhaustive
list of all tax lots in NYC for every year between 2003-2017, whether
that building was sold, for what amount, and several other additional
variables. A description of all variables can be seen in Table
\ref{tab:descripTable}.

\begin{table}

\caption{\label{tab:descripTable}\label{tab:descripTable} Description of Base Data}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llllllllll}
\toprule
variable & type & nobs & mean & sd & mode & min & max & median & n\_missing\\
\midrule
Annual\_Sales & Numeric & 12,012,780 & 2 & 8 & NA & 1 & 2,591 & 1 & 11,208,593\\
AssessLand & Numeric & 12,012,780 & 93,493 & 2,870,654 & 103,050 & 0 & 2,146,387,500 & 10,348 & 65\\
AssessTot & Numeric & 12,012,780 & 302,375 & 4,816,339 & 581,400 & 0 & 2,146,387,500 & 25,159 & 1,703,150\\
BldgArea & Numeric & 12,012,780 & 6,228 & 70,161 & 18,965 & 0 & 49,547,830 & 2,050 & 45\\
BldgDepth & Numeric & 12,012,780 & 46 & 34 & 50 & 0 & 9,388 & 42 & 44\\
\addlinespace
BldgFront & Numeric & 12,012,780 & 25 & 33 & 100 & 0 & 9,702 & 20 & 44\\
Block & Numeric & 12,012,780 & 5,297 & 3,695 & 1 & 0 & 71,724 & 4,799 & 44\\
BoroCode & Numeric & 12,012,780 & 3 & 1 & 5 & 1 & 5 & 4 & 47\\
BsmtCode & Numeric & 12,012,780 & 2 & 2 & 0 & 0 & 3,213 & 2 & 859,406\\
BuiltFAR & Numeric & 12,012,780 & 1 & 10 & 3 & 0 & 8,695 & 1 & 850,554\\
\addlinespace
ComArea & Numeric & 12,012,780 & 2,160 & 58,192 & 18,965 & 0 & 27,600,000 & 0 & 44\\
CommFAR & Numeric & 12,012,780 & 0 & 1 & 3 & 0 & 15 & 0 & 7,716,603\\
CondoNo & Numeric & 12,012,780 & 8 & 126 & 0 & 0 & 30,000 & 0 & 1,703,113\\
Easements & Numeric & 12,012,780 & 0 & 2 & 0 & 0 & 7,500 & 0 & 48\\
ExemptLand & Numeric & 12,012,780 & 37,073 & 2,718,194 & 0 & 0 & 2,146,387,500 & 1,290 & 65\\
\addlinespace
ExemptTot & Numeric & 12,012,780 & 107,941 & 3,522,172 & 0 & 0 & 2,146,387,500 & 1,360 & 1,703,149\\
FacilFAR & Numeric & 12,012,780 & 2 & 2 & 5 & 0 & 15 & 2 & 7,716,603\\
FactryArea & Numeric & 12,012,780 & 126 & 3,890 & 0 & 0 & 1,324,592 & 0 & 850,555\\
GarageArea & Numeric & 12,012,780 & 130 & 5,154 & 0 & 0 & 2,677,430 & 0 & 850,554\\
GROSS SQUARE FEET & Numeric & 12,012,780 & 4,423 & 45,691 & NA & 0 & 14,962,152 & 1,920 & 11,217,669\\
\addlinespace
lat & Numeric & 12,012,780 & 41 & 0 & 41 & 40 & 41 & 41 & 427,076\\
lon & Numeric & 12,012,780 & -74 & 0 & -74 & -78 & -74 & -74 & 427,076\\
Lot & Numeric & 12,012,780 & 115 & 655 & 10 & 0 & 9,999 & 38 & 44\\
LotArea & Numeric & 12,012,780 & 7,852 & 362,618 & 5,716 & 0 & 214,755,710 & 2,514 & 44\\
LotDepth & Numeric & 12,012,780 & 104 & 69 & 84 & 0 & 9,999 & 100 & 45\\
\addlinespace
LotFront & Numeric & 12,012,780 & 40 & 74 & 113 & 0 & 9,999 & 25 & 44\\
LotType & Numeric & 12,012,780 & 5 & 1 & 5 & 0 & 9 & 5 & 865,340\\
NumBldgs & Numeric & 12,012,780 & 1 & 4 & 1 & 0 & 2,740 & 1 & 46\\
NumFloors & Numeric & 12,012,780 & 2 & 2 & 4 & 0 & 300 & 2 & 44\\
OfficeArea & Numeric & 12,012,780 & 742 & 21,566 & 0 & 0 & 5,009,319 & 0 & 850,556\\
\addlinespace
OtherArea & Numeric & 12,012,780 & 673 & 49,848 & 0 & 0 & 27,600,000 & 0 & 850,555\\
ProxCode & Numeric & 12,012,780 & 1 & 2 & 1 & 0 & 5,469 & 1 & 197,927\\
ResArea & Numeric & 12,012,780 & 3,921 & 31,882 & 0 & 0 & 35,485,021 & 1,776 & 44\\
ResidFAR & Numeric & 12,012,780 & 1 & 1 & 2 & 0 & 12 & 1 & 7,716,603\\
RetailArea & Numeric & 12,012,780 & 309 & 14,394 & 6,965 & 0 & 21,999,988 & 0 & 850,554\\
\addlinespace
SALE PRICE & Numeric & 12,012,780 & 884,036 & 13,757,706 & NA & 0 & 4,111,111,766 & 319,000 & 11,208,593\\
sale\_psf & Numeric & 12,012,780 & 220 & 5,153 & NA & 0 & 1,497,500 & 114 & 11,250,396\\
SALE\_YEAR & Numeric & 12,012,780 & 2,009 & 5 & NA & 2,003 & 2,017 & 2,009 & 11,208,593\\
Sold & Numeric & 12,012,780 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
StrgeArea & Numeric & 12,012,780 & 169 & 5,810 & 12,000 & 0 & 1,835,150 & 0 & 850,554\\
\addlinespace
TOTAL\_SALES & Numeric & 12,012,780 & 884,036 & 13,757,706 & NA & 0 & 4,111,111,766 & 319,000 & 11,208,593\\
UnitsRes & Numeric & 12,012,780 & 4 & 36 & 0 & 0 & 20,811 & 1 & 45\\
UnitsTotal & Numeric & 12,012,780 & 4 & 42 & 1 & 0 & 44,276 & 2 & 47\\
Year & Numeric & 12,012,780 & 2,010 & 4 & 2,017 & 2,003 & 2,017 & 2,011 & 0\\
YearAlter1 & Numeric & 12,012,780 & 159 & 540 & 2,000 & 0 & 2,017 & 0 & 45\\
\addlinespace
YearAlter2 & Numeric & 12,012,780 & 20 & 202 & 0 & 0 & 2,017 & 0 & 48\\
YearBuilt & Numeric & 12,012,780 & 1,830 & 449 & 1,884 & 0 & 2,040 & 1,930 & 47\\
ZipCode & Numeric & 12,012,780 & 11,007 & 537 & 10,301 & 0 & 11,697 & 11,221 & 59,956\\
Address & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 17,902\\
AssessTotal & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 10,309,712\\
\addlinespace
bbl & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 0\\
BldgClass & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 16,372\\
Borough & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 0\\
BUILDING CLASS AT PRESENT & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,219,514\\
BUILDING CLASS AT TIME OF SALE & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,208,593\\
\addlinespace
BUILDING CLASS CATEGORY & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,208,765\\
Building\_Type & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 16,372\\
CornerLot & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,163,751\\
ExemptTotal & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 10,309,712\\
FAR & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,162,270\\
\addlinespace
IrrLotCode & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 16,310\\
MaxAllwFAR & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 4,296,221\\
OwnerName & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 137,048\\
OwnerType & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 10,445,328\\
TAX CLASS AT PRESENT & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,219,514\\
\addlinespace
TAX CLASS AT TIME OF SALE & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,208,593\\
ZoneDist1 & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 18,970\\
ZoneDist2 & Character & 12,012,780 & NA & NA & NA & NA & NA & NA & 11,715,653\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{global-filtering-of-the-data}{%
\subsubsection{Global filtering of the
data}\label{global-filtering-of-the-data}}

We only included building categories of significant interest in our
initial modeling data. Generally speaking, by significant interest we
are referring to building types that are regularly bought and sold on
the free market. These include residences, office buildings and
industrial buildings, and exclude things like government-owned buildings
and hospitals. We also excluded hotels as they tend to be comparatively
rare in the data and exhibit unique sales characteristics. The included
building types are displayed in Table \ref{tab:categoryTable}.

\begin{table}

\caption{\label{tab:unnamed-chunk-6}\label{tab:categoryTable} Included Building Cateogory Codes}
\centering
\begin{tabular}[t]{ll}
\toprule
Category & Description\\
\midrule
A & ONE FAMILY DWELLINGS\\
B & TWO FAMILY DWELLINGS\\
C & WALK UP APARTMENTS\\
D & ELEVATOR APARTMENTS\\
F & FACTORY AND INDUSTRIAL BUILDINGS\\
\addlinespace
G & GARAGES AND GASOLINE STATIONS\\
L & LOFT BUILDINGS\\
O & OFFICES\\
\bottomrule
\end{tabular}
\end{table}

The data were further filtered to include only records with equal to or
less than 2 buildings per tax lot, effectively excluding large outliers
in the data such as the World Trade Center and Stuyvesant Town. The
global filtering of the dataset reduces the base modeling data from
12,012,780 records down to 8,247,499, retaining 68.6\%\% of the original
data.

\hypertarget{exploratory-data-analysis}{%
\subsubsection{Exploratory Data
Analysis}\label{exploratory-data-analysis}}

The data contain building and sale records across the five boroughs of
New York City for the years 2003-2017. One challenge with creating a
predictive model of real estate sales data is the heterogeneity within
the data in terms of frequency of sales and sale price. These two
metrics (sale occurrence and amount) vary meaningfully across year,
borough and building classes (among other attributes). Table
\ref{tab:by_year} displays statistics which describe the base dataset
(pre-filtered) by year. Note how the frequency of transactions (\# of
Sales) and the sale amount (Median Sale \$/SF) tend to covary,
particularly through the downturn of 2009-2012. This covariance may be
due to the fact that the relative size of transactions tends to decrease
as capital becomes more constrained.

\begin{table}

\caption{\label{tab:by_year}\label{tab:by_year} Sales By Year}
\centering
\begin{tabular}[t]{rrrll}
\toprule
Year & N & \# Sales & Median Sale & Median Sale \$/SF\\
\midrule
2003 & 850515 & 78919 & \$218,000 & \$79.37\\
2004 & 852563 & 81794 & \$292,000 & \$124.05\\
2005 & 854862 & 77815 & \$360,500 & \$157.76\\
2006 & 857473 & 70928 & \$400,000 & \$168.07\\
2007 & 860480 & 61880 & \$385,000 & \$139.05\\
\addlinespace
2009 & 860519 & 43304 & \$245,000 & \$41.25\\
2010 & 860541 & 41826 & \$273,000 & \$75.35\\
2011 & 860320 & 40852 & \$263,333 & \$56.99\\
2012 & 859329 & 47036 & \$270,708 & \$52.72\\
2013 & 859372 & 50408 & \$315,000 & \$89.44\\
\addlinespace
2014 & 858914 & 51386 & \$350,000 & \$115.71\\
2015 & 859464 & 53208 & \$375,000 & \$135.62\\
2016 & 859205 & 53772 & \$385,530 & \$147.06\\
2017 & 859223 & 51059 & \$430,000 & \$171.71\\
\bottomrule
\end{tabular}
\end{table}

We observe similar variances across asset types. Table
\ref{tab:by_class} shows all buildings classes in the 2003-2017 period.
Unsurprisingly, residences tend to have the highest volume of sales
while offices tend to have the highest sale prices.

\begin{table}

\caption{\label{tab:by_class}\label{tab:by_class} Sales By Asset Class}
\centering
\begin{tabular}[t]{llrrll}
\toprule
Bldg Code & Build Type & N & \# Sales & Median Sale & Median Sale \$/SF\\
\midrule
A & One Family Dwellings & 4435615 & 252283 & \$320,000 & \$215.85\\
B & Two Family Dwellings & 3431762 & 219492 & \$340,000 & \$155.79\\
C & Walk Up Apartments & 1873447 & 135203 & \$330,000 & \$67.20\\
D & Elevator Apartments & 188689 & 45635 & \$398,000 & \$4.69\\
E & Warehouses & 84605 & 5126 & \$200,000 & \$31.48\\
\addlinespace
F & Factory & 67174 & 4440 & \$350,000 & \$56.44\\
G & Garages & 221620 & 13965 & \$0 & \$78.57\\
H & Hotels & 10807 & 619 & \$5,189,884 & \$184.82\\
I & Hospitals & 17650 & 687 & \$600,000 & \$62.66\\
J & Theatres & 2662 & 152 & \$113,425 & \$4.01\\
\addlinespace
K & Retail & 265101 & 14841 & \$200,000 & \$60.63\\
L & Loft & 18239 & 1259 & \$1,937,500 & \$101.36\\
M & Religious & 78063 & 1320 & \$375,000 & \$91.78\\
N & Asylum & 8498 & 190 & \$275,600 & \$35.90\\
O & Office & 93973 & 5294 & \$550,000 & \$143.29\\
\addlinespace
P & Public Assembly & 15292 & 437 & \$350,000 & \$85.47\\
Q & Recreation & 55193 & 232 & \$0 & \$0\\
R & Condo & 78188 & 40157 & \$444,750 & \$12.65\\
S & Mixed Use Residence & 467555 & 29396 & \$250,000 & \$78.29\\
T & Transportation & 4012 & 49 & \$0 & \$0\\
\addlinespace
U & Utility & 32802 & 129 & \$0 & \$175\\
V & Vacant & 449667 & 29091 & \$0 & \$134.70\\
W & Educational & 38993 & 704 & \$0 & \$0\\
Y & Gov't & 7216 & 44 & \$21,451.50 & \$0.30\\
Z & Misc & 49583 & 2740 & \$0 & \$0\\
\bottomrule
\end{tabular}
\end{table}

Sale price per square foot, in particular, varies considerably across
geography and asset class. Table \ref{tab:by_class_boro} shows the
breakdown of sales prices by borough and asset class. Manhattan tends to
command the highest sale-price-per-square-foot across asset types.
``Commercial'' asset types such as Office and Elevator Apartments tend
to fetch much lower price-per-square foot than do residential classes
such as one and two-family dwellings. Table \ref{tab:by_class_boro_num}
shows the number of transactions across the same dimensions.

\begin{table}

\caption{\label{tab:by_class_boro}\label{tab:by_class_boro} Sale Price Per Square Foot by Asset Class and Borough}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llllll}
\toprule
Build Type & BK & BX & MN & QN & SI\\
\midrule
Elevator Apartments & \$2.65 & \$1.74 & \$10.80 & \$1.87 & \$1.23\\
Factory & \$33.33 & \$53.19 & \$135.62 & \$92.42 & \$55.01\\
Garages & \$78.94 & \$80.57 & \$94.43 & \$71.11 & \$67.46\\
Loft & \$46.32 & \$78.26 & \$141.56 & \$150.37 & \$61.82\\
Office & \$118.52 & \$123.04 & \$225.96 & \$148.45 & \$105\\
\addlinespace
One Family Dwellings & \$221.26 & \$176.98 & \$757.58 & \$232.69 & \$203.88\\
Two Family Dwellings & \$140.95 & \$131.06 & \$296.10 & \$181.84 & \$160.76\\
Walk Up Apartments & \$69.97 & \$84.05 & \$50.61 & \$36.94 & \$75.38\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}

\caption{\label{tab:by_class_boro_num}\label{tab:by_class_boro_num} Number of Sales by Asset Class and Borough}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llllll}
\toprule
Build Type & BK & BX & MN & QN & SI\\
\midrule
Elevator Apartments & 8,377 & 4,252 & 23,641 & 9,196 & 169\\
Factory & 2,265 & 453 & 109 & 1,520 & 93\\
Garages & 5,386 & 2,659 & 1,097 & 4,000 & 823\\
Loft & 119 & 21 & 1,108 & 8 & 3\\
Office & 1,112 & 340 & 2,081 & 1,162 & 599\\
\addlinespace
One Family Dwellings & 45,009 & 17,508 & 1,654 & 126,333 & 61,779\\
Two Family Dwellings & 83,547 & 25,920 & 1,566 & 83,940 & 24,519\\
Walk Up Apartments & 63,552 & 18,075 & 19,824 & 31,932 & 1,820\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

\hypertarget{base-modeling-data}{%
\subsubsection{Base Modeling Data}\label{base-modeling-data}}

The base modeling dataset was constructed by combining several
open-source data repositories, outlined in the Data Sources section. In
addition to the data provided by New York City, several additional
features were engineered and appended to the base data. A summary table
of the additional features is presented in Table
\ref{tab:baseModelDataFeats}. A binary variable was created to indicate
whether a tax lot had a building on it (i.e., whether it was an empty
plot of land). In addition, building types were quantified by what
percent of their square footage belonged to the major property types:
Commercial, Residential, Office, Retail, Garage, Storage, Factory and
Other.

\begin{table}

\caption{\label{tab:Table 1}\label{tab:baseModelDataFeats} Base Modeling Data Features}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllll}
\toprule
Feature & Min & Median & Mean & Max\\
\midrule
has\_building\_area & 0 & 1.00 & 1.00 & 1.00\\
Percent\_Com & 0 & 0.00 & 0.16 & 1.00\\
Percent\_Res & 0 & 1.00 & 0.82 & 1.00\\
Percent\_Office & 0 & 0.00 & 0.07 & 1.00\\
Percent\_Retail & 0 & 0.00 & 0.04 & 1.00\\
\addlinespace
Percent\_Garage & 0 & 0.00 & 0.01 & 1.00\\
Percent\_Storage & 0 & 0.00 & 0.02 & 1.00\\
Percent\_Factory & 0 & 0.00 & 0.00 & 1.00\\
Percent\_Other & 0 & 0.00 & 0.00 & 1.00\\
Last\_Sale\_Price & 0 & 312.68 & 531.02 & 62,055.59\\
\addlinespace
Last\_Sale\_Price\_Total & 2 & 2,966,835.00 & 12,844,252.00 & 1,932,900,000.00\\
Years\_Since\_Last\_Sale & 1 & 4.00 & 5.05 & 14.00\\
SMA\_Price\_2\_year & 0 & 296.92 & 500.89 & 62,055.59\\
SMA\_Price\_3\_year & 0 & 294.94 & 495.29 & 62,055.59\\
SMA\_Price\_5\_year & 0 & 300.12 & 498.82 & 62,055.59\\
\addlinespace
Percent\_Change\_SMA\_2 & -1 & 0.00 & 685.69 & 15,749,999.50\\
Percent\_Change\_SMA\_5 & -1 & 0.00 & 337.77 & 6,299,999.80\\
EMA\_Price\_2\_year & 0 & 288.01 & 482.69 & 62,055.59\\
EMA\_Price\_3\_year & 0 & 283.23 & 471.98 & 62,055.59\\
EMA\_Price\_5\_year & 0 & 278.67 & 454.15 & 62,055.59\\
\addlinespace
Percent\_Change\_EMA\_2 & -1 & 0.00 & 422.50 & 9,415,128.85\\
Percent\_Change\_EMA\_5 & -1 & 0.06 & 308.05 & 5,341,901.60\\
\bottomrule
\end{tabular}}
\end{table}

Importantly, two variables were created from the Sale Prices: A
price-per-square-foot-figure (``Sale\_Price'') and a total Sale Price
(``Sale\_Price\_Total''). Sale Price per square foot eventually became
the outcome variable in one of the predictive models. We then create a
feature to carry forward the previous sale price of a tax lot, if there
was one, through successive years. Previous Sale Price was then used to
create Simple Moving Averages (SMA), Exponential Moving Averages (EMA),
and percent change measurements between the moving averages. In total,
69 variables were input to the feature engineering process and 92
variables were output. The final base modeling dataset was 92 variables
by 8,247,499 rows.

\hypertarget{zip-code-modeling-data}{%
\subsubsection{Zip Code Modeling Data}\label{zip-code-modeling-data}}

The first of the two comparative modeling datasets was the Zip Code
modeling data. Using the base data as a starting point, several features
were generated to describe characteristics of the Zip Code where each
tax lot resides. A summary table of the Zip Code level features is
presented in \ref{tab:zipcodemodelfeats}.

\begin{table}

\caption{\label{tab:Table 2}\label{tab:zipcodemodelfeats} Zip Code Modeling Data Features}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllll}
\toprule
Feature & Min & Median & Mean & Max\\
\midrule
Last Year Zip Sold & 0.00 & 27.00 & 31.14 & 112.00\\
Last Year Zip Sold Percent Ch & -1.00 & 0.00 &  & \\
Last Sale Price zip code average & 0.00 & 440.95 & 522.87 & 1,961.21\\
Last Sale Price Total zip code average & 10.00 & 5,312,874.67 & 11,877,688.55 & 1,246,450,000.00\\
Last Sale Date zip code average & 12,066.00 & 13,338.21 & 13,484.39 & 17,149.00\\
\addlinespace
Years Since Last Sale zip code average & 1.00 & 4.84 & 4.26 & 11.00\\
SMA Price 2 year zip code average & 34.31 & 429.26 & 501.15 & 2,092.41\\
SMA Price 3 year zip code average & 34.31 & 422.04 & 496.47 & 2,090.36\\
SMA Price 5 year zip code average & 39.48 & 467.04 & 520.86 & 2,090.36\\
Percent Change SMA 2 zip code average & -0.20 & 0.04 & 616.47 & 169,999.90\\
\addlinespace
Percent Change SMA 5 zip code average & -0.09 & 0.03 & 341.68 & 113,333.27\\
EMA Price 2 year zip code average & 30.77 & 401.43 & 479.38 & 1,883.81\\
EMA Price 3 year zip code average & 33.48 & 419.11 & 479.95 & 1,781.38\\
EMA Price 5 year zip code average & 29.85 & 431.89 & 472.80 & 1,506.46\\
Percent Change EMA 2 zip code average & -0.16 & 0.06 & 388.90 & 107,368.37\\
\addlinespace
Percent Change EMA 5 zip code average & -0.08 & 0.07 & 326.17 & 107,368.38\\
Last Sale Price bt only & 0.00 & 357.71 & 485.97 & 6,401.01\\
Last Sale Price Total bt only & 10.00 & 3,797,461.46 & 11,745,130.56 & 1,246,450,000.00\\
Last Sale Date bt only & 12,055.00 & 13,331.92 & 13,497.75 & 17,149.00\\
Years Since Last Sale bt only & 1.00 & 4.78 & 4.30 & 14.00\\
\addlinespace
SMA Price 2 year bt only & 0.00 & 347.59 & 462.67 & 5,519.39\\
SMA Price 3 year bt only & 0.00 & 345.40 & 458.50 & 5,104.51\\
SMA Price 5 year bt only & 0.00 & 372.30 & 481.09 & 4,933.05\\
Percent Change SMA 2 bt only & -0.55 & 0.03 & 600.10 & 425,675.69\\
Percent Change SMA 5 bt only & -0.33 & 0.02 & 338.15 & 188,888.78\\
\addlinespace
EMA Price 2 year bt only & 0.00 & 332.98 & 442.79 & 5,103.51\\
EMA Price 3 year bt only & 0.00 & 332.79 & 443.02 & 4,754.95\\
EMA Price 5 year bt only & 0.00 & 340.57 & 436.70 & 4,270.37\\
Percent Change EMA 2 bt only & -0.47 & 0.06 & 377.17 & 254,462.97\\
Percent Change EMA 5 bt only & -0.34 & 0.06 & 335.17 & 178,947.30\\
\bottomrule
\end{tabular}}
\end{table}

In general, the base model data features were aggregated to a Zip Code
level and appended, including the SMA, EMA and percent change
calculations. Additionally, a second set of features were added, denoted
as ``bt\_only'', which aggregated the data in a similar fashion but only
included tax lots of the same building type in the calculations. In
total, the Zip Code feature engineering process input 92 variables and
output 122 variables.

\hypertarget{spatial-lag-modeling-data}{%
\subsubsection{Spatial Lag Modeling
Data}\label{spatial-lag-modeling-data}}

Spatial lags are variables created from physically proximate
observations. For example, calculating the average age of all buildings
within 100 meters of a tax lot constitutes a spatial lag. Creating
spatial lags presents both advantages and disadvantages in the modeling
process. Spatial lags allow for much more fine-tuned measurements of a
building's surrounding area. Intuitively, knowing the average sale price
of all buildings within 500 meters of a building can be more informative
than knowing the sale prices of all buildings in the same Zip Code.
However, creating spatial lags is computationally expensive. In
addition, it can be challenging to set a proper radius for the spatial
lag calculation; in a city, 500 meters may be appropriate (for certain
building types), whereas several kilometers or more may be appropriate
for less densely populated areas. In this paper, we present a solution
for the computational challenges and suggest a potential approach to
solving the radius-choice problem.

\hypertarget{creating-the-point-neighbor-relational-graph}{%
\paragraph{Creating the Point-Neighbor Relational
Graph}\label{creating-the-point-neighbor-relational-graph}}

To build our spatial lags, for each point in the data, we must identify
which of all other points in the data fall within a specified radius.
This neighbor identification process requires iteratively running
point-in-polygon operations, i.e., ``given polygon P and an arbitrary
point q, determine whether point q is enclosed by the edges of the
polygon'' (Huang 1996). This process is conceptually illustrated in
figure \ref{fig:Spatial Lag Feataure Process}.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/Spatial Lag Creation} \caption{Spatial Lag Feature Creation Process}\label{fig:Spatial Lag Feataure Process}
\end{figure}

Given that, for every point \(q_i\) in our dataset, we need to determine
whether every other point \(q_i\) falls within a given radius, this
means that we can approximate the time-complexity of our operation as:

\[
O(N(N-1))
\]

Since the number of operations approaches \(N^2\), calculating spatial
lags for all 8,247,499 observations in our modeling data would be
infeasible from a time and computation perspective. Assuming that tax
lots rarely if ever move over time, we first reduced the task to the
number of unique tax lots in New York City from 2003-2017, which is
514,124 points. Next, we implemented an indexing technique that greatly
speeds up the process of creating a point-neighbor relational graph. The
indexing technique both reduces the relative search space for each
computation and also allows for parallelization of the point-in-polygon
operations by dividing the data into a gridded space. The gridded
spatial indexing process is outlined in Algorithm \ref{alg:spatial1}.

\begin{algorithm}
  \caption{Gridded Spatial Indexing}
  \label{alg:spatial1}
  \begin{algorithmic}[1]
      \For{\texttt{each grid partition $G$}}
        \State \texttt{Extract all points points $G_i$ contained within partition $G$}
        \State \texttt{Calculate convex hull $H(G)$ such that the buffer extends to distance $d$}
        \State \texttt{Define Search space $S$ as all points within Convex hull $H(G)$}
        \State \texttt{Extract all points $S_i$ contained within $S$}
          \For{\texttt{each data point $G_i$}}
            \State \texttt{Identify all points points in $S_i$ that fall within $abs(G_i+d)$}
        \EndFor
      \EndFor
  \end{algorithmic}
\end{algorithm}

\noindent Each gridded partition of the data is married with a
corresponding search space \(S\), which is the convex hull of the
partition space buffered by the maximum distance \(d\). In our case, we
buffered the search space by 500 meters. Choosing an appropriate radius
for buffering presents an additional challenge in creating
spatially-conscious machine learning predictive models. In this paper,
we chose an arbitrary radius, and use a two-stage modeling process to
test the appropriateness of that assumption. Future work may want to
explore implementing an ``adaptive bandwidth'' technique using
cross-validation to determine the optimal radius.

By partitioning the data into spatial grids, we were able to reduce the
search space for each operation by an arbitrary number of partitions
\(G\). This improves the base run-time complexity to:

\[
O(N(\frac{N-1}{G})
\]

\noindent By making G arbitrarily large (bounded by computational
resources only), we reduced the runtime substantially. Furthermore,
binning the operations into grids allowed us to parallelize the
computation, further reducing the overall runtime. Figure
\ref{fig:Spatial Indexing Process} shows a comparison of computation
times between the basic point-in-polygon technique and a sequential
version of the grided indexing technique. Note that the grid method
starts as slower than the basic point-in-polygon technique due to
pre-processing overhead, but quickly wins out in terms of speed as the
complexity of the task increases. This graph also does not reflect the
parallelization of the grid method, which further reduced the time
required to calculate the point-neighbor relational graph.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/Example Spatial Indexing Techniques} \caption{Spatial Index Time Comparison}\label{fig:Spatial Indexing Process}
\end{figure}

\hypertarget{calculating-spatial-lags}{%
\paragraph{Calculating Spatial Lags}\label{calculating-spatial-lags}}

Once we contructed the point-neighbor relational graph, we then used the
graph to aggregate the data into spatial lag variables. One advantage of
using spatial lags is the abundant number of potential features which
can be engineered. Spatial lags can be weighted based on a distance
function, e.g., physically closer observations can be given more weight.
For our modeling purposes, we created two sets of features:
inverse-distance weighted features (denoted with a "\_dist" in Table
\ref{tab:SpLAgFeats}) and simple average features (denoted with
"\_basic" in Table \ref{tab:SpLAgFeats}).

Temporal and spatial derivatives of the Spatial Lag features, presented
in Table \ref{tab:SpLAgFeats}, were also added to the model, including:
variables weighted by Euclidean distance (``dist''), basic averages of
the spatial lag radius (``basic mean''), SMA for 2 years, 3 years and 5
years, EMA for 2 years, 3 years and 5 years, and year-over-year percent
changes for all variables (``perc change''). In total, the spatial lag
feature engineering process input 92 variables and output 194 variables.

\begin{table}

\caption{\label{tab:SpLagFeats}\label{tab:SpLAgFeats} All Spatial Lag Features}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllll}
\toprule
Feature & Min & Median & Mean & Max\\
\midrule
Radius\_Total\_Sold\_In\_Year & 1.00 & 20.00 & 24.00 & 201.00\\
Radius\_Average\_Years\_Since\_Last\_Sale & 1.00 & 4.43 & 4.27 & 14.00\\
Radius\_Res\_Units\_Sold\_In\_Year & 0.00 & 226.00 & 289.10 & 2,920.00\\
Radius\_All\_Units\_Sold\_In\_Year & 0.00 & 255.00 & 325.94 & 2,923.00\\
Radius\_SF\_Sold\_In\_Year & 0.00 & 259,403.00 & 430,891.57 & 8,603,639.00\\
\addlinespace
Radius\_Total\_Sold\_In\_Year\_sum\_over\_2\_years & 2.00 & 41.00 & 48.15 & 256.00\\
Radius\_Average\_Years\_Since\_Last\_Sale\_sum\_over\_2\_years & 2.00 & 9.25 & 8.70 & 26.00\\
Radius\_Res\_Units\_Sold\_In\_Year\_sum\_over\_2\_years & 0.00 & 493.00 & 584.67 & 3,397.00\\
Radius\_All\_Units\_Sold\_In\_Year\_sum\_over\_2\_years & 1.00 & 555.00 & 660.67 & 4,265.00\\
Radius\_SF\_Sold\_In\_Year\_sum\_over\_2\_years & 2,917.00 & 580,947.00 & 872,816.44 & 14,036,469.00\\
\addlinespace
Radius\_Total\_Sold\_In\_Year\_percent\_change & -0.99 & 0.00 & 0.27 & 77.00\\
Radius\_Average\_Years\_Since\_Last\_Sale\_percent\_change & -0.91 & 0.13 & 0.26 & 8.00\\
Radius\_Res\_Units\_Sold\_In\_Year\_percent\_change & -1.00 & -0.04 &  & \\
Radius\_All\_Units\_Sold\_In\_Year\_percent\_change & -1.00 & -0.04 &  & \\
Radius\_SF\_Sold\_In\_Year\_percent\_change & -1.00 & -0.02 &  & \\
\addlinespace
Radius\_Total\_Sold\_In\_Year\_sum\_over\_2\_years\_percent\_change & -0.96 & -0.03 & 0.03 & 15.00\\
Radius\_Average\_Years\_Since\_Last\_Sale\_sum\_over\_2\_years\_percent\_change & -0.72 & 0.12 & 0.17 & 2.50\\
Radius\_Res\_Units\_Sold\_In\_Year\_sum\_over\_2\_years\_percent\_change & -1.00 & -0.04 &  & \\
Radius\_All\_Units\_Sold\_In\_Year\_sum\_over\_2\_years\_percent\_change & -0.99 & -0.04 & 0.12 & 84.00\\
Radius\_SF\_Sold\_In\_Year\_sum\_over\_2\_years\_percent\_change & -0.98 & -0.04 & 0.18 & 361.55\\
\addlinespace
Percent\_Com\_dist & 0.00 & 0.04 & 0.07 & 0.56\\
Percent\_Res\_dist & 0.00 & 0.46 & 0.43 & 0.66\\
Percent\_Office\_dist & 0.00 & 0.01 & 0.03 & 0.48\\
Percent\_Retail\_dist & 0.00 & 0.02 & 0.02 & 0.09\\
Percent\_Garage\_dist & 0.00 & 0.00 & 0.00 & 0.27\\
\addlinespace
Percent\_Storage\_dist & 0.00 & 0.00 & 0.01 & 0.26\\
Percent\_Factory\_dist & 0.00 & 0.00 & 0.00 & 0.04\\
Percent\_Other\_dist & 0.00 & 0.00 & 0.00 & 0.09\\
Percent\_Com\_basic\_mean & 0.00 & 0.04 & 0.07 & 0.54\\
Percent\_Res\_basic\_mean & 0.00 & 0.46 & 0.43 & 0.66\\
\addlinespace
Percent\_Office\_basic\_mean & 0.00 & 0.01 & 0.03 & 0.44\\
Percent\_Retail\_basic\_mean & 0.00 & 0.02 & 0.02 & 0.08\\
Percent\_Garage\_basic\_mean & 0.00 & 0.00 & 0.00 & 0.29\\
Percent\_Storage\_basic\_mean & 0.00 & 0.00 & 0.01 & 0.23\\
Percent\_Factory\_basic\_mean & 0.00 & 0.00 & 0.00 & 0.03\\
\addlinespace
Percent\_Other\_basic\_mean & 0.00 & 0.00 & 0.00 & 0.04\\
Percent\_Com\_dist\_perc\_change & -0.90 & 0.00 & 0.00 & 6.18\\
Percent\_Res\_dist\_perc\_change & -0.50 & 0.00 & 0.03 & 36.73\\
Percent\_Office\_dist\_perc\_change & -1.00 & 0.00 &  & \\
Percent\_Retail\_dist\_perc\_change & -0.82 & 0.00 &  & \\
\addlinespace
Percent\_Garage\_dist\_perc\_change & -1.00 & 0.00 &  & \\
Percent\_Storage\_dist\_perc\_change & -1.00 & -0.01 &  & \\
Percent\_Factory\_dist\_perc\_change & -1.00 & 0.00 &  & \\
Percent\_Other\_dist\_perc\_change & -1.00 & 0.00 &  & \\
SMA\_Price\_2\_year\_dist & 0.00 & 400.01 & 496.30 & 3,816.57\\
\addlinespace
SMA\_Price\_3\_year\_dist & 0.00 & 396.94 & 492.00 & 3,816.57\\
SMA\_Price\_5\_year\_dist & 8.83 & 425.55 & 515.29 & 3,877.53\\
Percent\_Change\_SMA\_2\_dist & -0.13 & 0.03 & 552.33 & 804,350.67\\
Percent\_Change\_SMA\_5\_dist & -0.09 & 0.02 & 317.46 & 322,504.58\\
EMA\_Price\_2\_year\_dist & 0.00 & 378.63 & 475.54 & 3,431.17\\
\addlinespace
EMA\_Price\_3\_year\_dist & 8.83 & 382.25 & 476.05 & 3,296.46\\
EMA\_Price\_5\_year\_dist & 7.88 & 386.34 & 468.91 & 2,813.34\\
Percent\_Change\_EMA\_2\_dist & -0.09 & 0.06 & 346.51 & 480,829.57\\
Percent\_Change\_EMA\_5\_dist & -0.02 & 0.06 & 303.55 & 273,458.42\\
SMA\_Price\_2\_year\_basic\_mean & 0.02 & 412.46 & 496.75 & 2,509.79\\
\addlinespace
SMA\_Price\_3\_year\_basic\_mean & 0.02 & 409.00 & 492.43 & 2,509.79\\
SMA\_Price\_5\_year\_basic\_mean & 17.16 & 443.34 & 515.67 & 2,621.01\\
Percent\_Change\_SMA\_2\_basic\_mean & -0.13 & 0.04 & 543.51 & 393,749.99\\
Percent\_Change\_SMA\_5\_basic\_mean & -0.09 & 0.03 & 312.46 & 157,500.00\\
EMA\_Price\_2\_year\_basic\_mean & 0.02 & 390.30 & 475.96 & 2,259.21\\
\addlinespace
EMA\_Price\_3\_year\_basic\_mean & 11.39 & 393.25 & 476.45 & 2,136.36\\
EMA\_Price\_5\_year\_basic\_mean & 15.30 & 402.06 & 469.09 & 1,848.27\\
Percent\_Change\_EMA\_2\_basic\_mean & -0.09 & 0.06 & 340.89 & 235,378.24\\
Percent\_Change\_EMA\_5\_basic\_mean & -0.02 & 0.06 & 296.78 & 133,547.59\\
SMA\_Price\_2\_year\_dist\_perc\_change & -0.74 & 0.05 & 0.17 & 10,540.56\\
\addlinespace
SMA\_Price\_3\_year\_dist\_perc\_change & -0.74 & 0.05 & 0.17 & 10,540.56\\
SMA\_Price\_5\_year\_dist\_perc\_change & -0.74 & 0.04 & 0.06 & 15.37\\
Percent\_Change\_SMA\_2\_dist\_perc\_change & -Inf & -0.24 & NaN & \\
Percent\_Change\_SMA\_5\_dist\_perc\_change & -Inf & -0.14 & NaN & \\
EMA\_Price\_2\_year\_dist\_perc\_change & -0.74 & 0.06 & 0.18 & 10,540.57\\
\addlinespace
EMA\_Price\_3\_year\_dist\_perc\_change & -0.73 & 0.06 & 0.08 & 15.06\\
EMA\_Price\_5\_year\_dist\_perc\_change & -0.63 & 0.06 & 0.07 & 12.04\\
Percent\_Change\_EMA\_2\_dist\_perc\_change & -Inf & -0.13 & NaN & \\
Percent\_Change\_EMA\_5\_dist\_perc\_change & -556.60 & -0.10 &  & \\
SMA\_Price\_2\_year\_basic\_mean\_perc\_change & -0.55 & 0.05 & 0.12 & 9,375.77\\
\addlinespace
SMA\_Price\_3\_year\_basic\_mean\_perc\_change & -0.55 & 0.05 & 0.11 & 9,375.77\\
SMA\_Price\_5\_year\_basic\_mean\_perc\_change & -0.50 & 0.04 & 0.06 & 5.90\\
Percent\_Change\_SMA\_2\_basic\_mean\_perc\_change & -Inf & -0.19 & NaN & \\
Percent\_Change\_SMA\_5\_basic\_mean\_perc\_change & -Inf & -0.12 & NaN & \\
EMA\_Price\_2\_year\_basic\_mean\_perc\_change & -0.53 & 0.06 & 0.12 & 9,375.78\\
\addlinespace
EMA\_Price\_3\_year\_basic\_mean\_perc\_change & -0.47 & 0.06 & 0.08 & 23.54\\
EMA\_Price\_5\_year\_basic\_mean\_perc\_change & -0.37 & 0.06 & 0.07 & 4.81\\
Percent\_Change\_EMA\_2\_basic\_mean\_perc\_change & -Inf & -0.13 & NaN & \\
Percent\_Change\_EMA\_5\_basic\_mean\_perc\_change & -136.59 & -0.11 &  & \\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{dependent-variables}{%
\subsection{Dependent Variables}\label{dependent-variables}}

The final step in creating the modeling data was to define the dependent
variables reflective of the prediction tasks; a binary variable for
classification and a continuous variable for regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Binary: Sold} whether a tax lot sold in a given year. Used in
  the Probability of Sale classification model.
\item
  \textbf{Continuous: Sale Price per SF} The price-per-square-foot
  associated with a transaction, if a sale took place. Used in the Sale
  Price Regression model.
\end{enumerate}

\noindent Table \ref{tab:OutcomeDistro} describes the distributions of
both outcome variables.

\begin{table}

\caption{\label{tab:table 4}\label{tab:OutcomeDistro} Distributions for Outcome Variables}
\centering
\begin{tabular}[t]{lll}
\toprule
  & Sold & Sale Price per SF\\
\midrule
Min. & 0.00 & 0.0\\
1st Qu. & 0.00 & 163.5\\
Median & 0.00 & 375.2\\
Mean & 0.04 & 644.8\\
3rd Qu. & 0.00 & 783.3\\
Max. & 1.00 & 83,598.7\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{algorithms-comparison}{%
\subsection{Algorithms Comparison}\label{algorithms-comparison}}

We implemented and compared several algorithms across our two-stage
process. In Stage 1, the Random Forest algorithm was used to identify
the optimal subset of building types and geographies for our spatial lag
aggregation assumptions. In Stage 2, we analyzed the hold-out test
performance of several algorithms including Random Forest, Generalized
Linear Model (GLM), Gradient Boosting Machine (GBM), and Feed-Forward
Artificial Neural Network (ANN). Each algorithm was run over the three
competing feature engineering datasets and for both the classification
and regression tasks.

\hypertarget{random-forest}{%
\subsubsection{Random Forest}\label{random-forest}}

Leo Breiman proposed the Random Forest concept in 2001 as an ensemble of
prediction decision trees iteratively trained across randomly generated
subsets of data (Breiman 2001). Algorithm \ref{alg:RandomForestAlo}
outlines the procedure (Hastie, Tibshirani, and Friedman 2001).

\begin{algorithm}
  \caption{Random Forest for Regression or Classification}\label{alg:RandomForestAlo}

\begin{enumerate}
  \item For $b = 1$ to $B$
  \begin{enumerate}
    \item Draw a bootstrap sample $Z$ of the size $N$ from the training data.
    \item Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached.
  \begin{enumerate}
    \item Select $m$ variables at random from the $p$ variables
    \item Pick the best variable/split-point among the $m$.
    \item Split the node into two daughter nodes.
  \end{enumerate}
  \end{enumerate}
  \item Output the ensemble of trees $\{T_b\}_1^B$.
\end{enumerate}

To make a prediction at a new point $x$:

\textit{Regression:} $\hat{f}_{rf}^B(x) = \frac{1}{B}\sum_{b=1}^{B}T_b(x)$

\textit{Classification:} Let $\hat{C_b}(x)$ be the class prediction of the $b$th random-forest tree. Then $\hat{C_{rf}^B}(x)=$ \textit{majority vote} $\{\hat{C}_{b}(x)\}_1^B $

\end{algorithm}

Previous works (see: Antipov and Pokryshevskaya (2012); also
Schernthanner H. (2016)) have found the Random Forest algorithm suitable
to prediction tasks involving real estate. While algorithms exist that
may outperform Random Forest in terms of predictive accuracy (such as
neural networks and functional gradient descent algorithms), Random
Forest is highly scalable and parallelizable, and is therefore an
attractive choice for quickly assessing the predictive power of
different feature engineering techniques. For these reasons and more
outlined below, we selected Random Forest as the primary algorithm for
Stage 1 of our modeling process.

Random Forest, like all predictive algorithms used in this paper, suits
both classification and regression tasks. The Random Forest algorithm
works by generating a large number of independent classification or
regression decision trees and then employing majority voting (for
classification) or averaging (for regression) to generate predictions.
Over a dataset of N rows by M predictors, a bootstrap sample of the data
is chosen (n \textless{} N) as well as a subset of the predictors (m
\textless{} M). Individual decision/regression trees are built on the n
by m sample. Because the trees develop independently (and not
sequentially, as is the case with most functional gradient descent
algorithms), the tree building process can be executed in parallel. With
a sufficiently large number of computer cores, the model training time
can be significantly reduced.

We chose Random Forest as the algorithm for Stage 1 because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The algorithm can be parallelized and is relatively fast compared to
  neural networks and functional gradient descent algorithms
\item
  Can accommodate categorical variables with many levels. Real estate
  data often contains information describing the location of the
  property, or the property itself, as one of a large set of possible
  choices, such as neighborhood, county, census tract, district,
  property type, and zoning information. Because factors need to be
  recoded as individual dummy variables in the model building process,
  factors with many levels will quickly encounter the curse of
  dimensionality in multiple regression techniques.
\item
  Appropriately handles missing data. Predictions can be made with the
  parts of the tree which are successfully built, and therefore, there
  is no need to filter out incomplete observations or impute missing
  values. Since much real estate data is self-reported, incomplete
  fields are common in the data.
\item
  Robust against outliers. Because of bootstrap sampling, outliers
  appear in individual trees less often, and therefore, their influence
  is curtailed. Real estate data, especially with regards to pricing,
  tends to contain outliers. For example, the dependent variable in one
  of our models, Sale Price, shows a clear divergence in median and
  mean, as well as a maximum significantly higher than the third
  quartile.
\item
  Can recognize non-linear relationships in data, which is useful when
  modeling spatial relationships.
\item
  Is not affected by co-linearity in the data. This is highly valuable
  as real estate data can be highly correlated.
\end{enumerate}

To run the model, we chose the h2o.randomForest implementation from the
h2o R open source library. The h2o implementation of the Random Forest
algorithm is particularly well-suited for high parallelization. For more
information, see: \url{https://www.h2o.ai/}.

\hypertarget{generalized-linear-model}{%
\subsubsection{Generalized Linear
Model}\label{generalized-linear-model}}

A generalized linear model (GLM) is an extension of the general linear
model that estimates an independent variable \(y\) as the linear
combination of one or more predictor variables. A GLM is made up of a
linear predictor taking the form
\(\eta_i = \beta_0+\beta_1x_{i1}+...+\beta_{p}x_{ip}\), a link function
that describes how the mean, \(E(Y_i)=\mu_i\) depends on the linear
predictor, and a variance function that describes how the variance,
var\((Y_i)\) depends on the mean (Hoffmann 2004). The observed value of
the dependent variable \(y\) for observation \(i\)
\((i = 1, 2, ..., n)\) is modeled as a linear function of \((p - 1)\)
independent variables \(x1, x2,... ,xp-1\) as

\[
y_i = \beta_0+\beta_1x_{i1}+...+\beta_{p-1}x_{i(p-1)}+e_i
\]

Several family types of GLM's exist. For a binary independent variable,
a binomial logistic regression is appropriate. For a continuous
independent variable, the Gaussian or another distribution is
appropriate. For our purposes, the Gaussian family is used for our
regression task and binomial for the classification.

\hypertarget{gradient-boosting-machine}{%
\subsubsection{Gradient Boosting
Machine}\label{gradient-boosting-machine}}

Gradient Boosting Machine (GBM) is one of the most popular machine
learning algorithms available today. The algorithm uses iteratively
refined approximations, obtained through cross-validation, to
incrementally increase predictive accuracy. Similar to Random Forest,
GBM is well-suited to using regression trees as the base learner.
Gradient boosting constructs additive regression models by sequentially
fitting a simple parameterized function (a ``base learner,'' in our
case, a regression tree) to ``pseudo''-residuals by least squares at
each iteration (Friedman 2002). The pseudo-residuals are the gradient of
the loss function being minimized, with respect to the model values at
each training data point evaluated at the current step. The tree-variant
of the generic Gradient Boosting Algorithm is outlined in algorithm
\ref{alg:GBMAlo} (Hastie, Tibshirani, and Friedman 2001).

\begin{algorithm}
  \caption{Gradient Tree Boosting Algorithm}\label{alg:GBMAlo}
\begin{enumerate}
\item Initialize: $f_0(x) = \arg\min_\gamma \sum_{i=1}^N L(y_i, \gamma).$
\item For $m$ = 1 to $M$:
  \begin{enumerate}
  \item For $1,2,...,N$ compute "pseudo-residuals":
  $$
   r_{im} = -\left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f=f_{m-1}(x)} \quad
  $$
  \item Fit a regression tree to the targets $r_{im}$ giving terminal regions $R_{jm}, j = 1,2,...J_m$
  \item For $j = 1,2,...,J_m$ compute:
  
  $$
  \gamma_{jm} = \underset{\gamma}{\operatorname{arg\,min}} \sum_{xi \in R_{jm}} L\left(y_i, f_{m-1}(x_i) + \gamma \right).
  $$
  \item Update $f_m(x) = F_{m-1}(x) + \sum_{j=1}^{J_m}\gamma_{jm}I(x \in R_{jm})$
  \end{enumerate}
\item Output $\hat{f}(x) = f_m(x)$
\end{enumerate}
\end{algorithm}

\hypertarget{feed-forward-artificial-neural-network}{%
\subsubsection{Feed-Forward Artificial Neural
Network}\label{feed-forward-artificial-neural-network}}

The Artificial Neural Network (ANN) implementation used in this paper is
a multi-layer feed-forward artificial neural network trained with
stochastic gradient descent using back-propagation. Common synonyms for
an ANN model are ``multi-layer perceptron'' and ``deep neural network.''
The feed-forward ANN is one of the most common neural network
algorithms, but other types exist, such as the Convolutional Neural
Network (CNN) which performs well on image classification tasks, and the
Recurrent Neural Network (RNN) which is well-suited for sequential data
such as text and audio (Schmidhuber 2015). The feed-forward ANN is
typically best suited for tabular data.

The neural network model has unknown parameters, often called weights,
and we seek values for them that make the model fit the training data
well (Hastie, Tibshirani, and Friedman 2001). We denote the complete set
of weights by \(\theta\), which consists of
\(\{\alpha_{0m},\alpha_m;m=1,2,...,M\} M(p+1)\) weights and
\(\{\beta_{0k},\beta_k;k=1,2,...,K\} K(p+1)\) weights.

For both classification and regression, we use sum-of-squared errors as
our measure of fit (error function)

\[
R(\theta)=\sum_{k=1}^K\sum_{i=1}^N(y_{ik}-f_{k}(x_i))^2
\] The generic approach to minimizing \(R(\theta)\) is by gradient
descent, called back-propagation in this setting (Hastie, Tibshirani,
and Friedman 2001). Because of the compositional form of the model, the
gradient can be derived using the chain rule for differentiation. This
can be computed by a forward and backward sweep over the network,
keeping track only of quantities local to each unit. Here is
back-propagation in detail for squared error loss:

\[
R(\theta) = \sum_{i=1}^NR_i
\] \[
\sum_{i=1}^N\sum_{k=1}^K(y_{ik}-f_{k}(x_i))^2
\]

For our implementations, we used the rectifier activation function with
1024 hidden layers, 100 epochs and L1 regularization set to 0.00001. The
implementation we chose was the h2o.deeplearning open source R library.
For more information, see: \url{https://www.h2o.ai/}.

\hypertarget{model-validation}{%
\subsection{Model Validation}\label{model-validation}}

The goal of our predictive modeling efforts were to be able to
successfully predict both the probability and amount of real estate
sales into the near future. As such, trained and evaluated our models
using use out-of-time validation to assess performance. As shown in
Figure \ref{fig:Train Test Validate} The models were trained using data
from 2003-2015. We used 2016 data during the model training process for
model validation purposes. Finally, we scored our models using 2017 data
as a hold-out sample. Using out-of-time validation ensured that our
models generalized well into the immediate future.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/Train Validate Test} \caption{Out-of-time validation}\label{fig:Train Test Validate}
\end{figure}

\hypertarget{evaluation-metrics}{%
\subsection{Evaluation Metrics}\label{evaluation-metrics}}

We chose evaluation metrics that allowed us to easily compare the
performance of the models against other similar models with the same
dependent variable. The classification models (Probability of Sale) were
compared using Area Under the ROC Curve (AUC). The regression models
(Sale Price) were compared using Root Mean Squared Error (RMSE). Both
evaluation metrics are common for their respective outcome variable
types, and as such were useful for comparing within model-groups.

\hypertarget{area-under-roc-curve}{%
\subsubsection{Area Under ROC Curve}\label{area-under-roc-curve}}

A classification model typically outputs a probability that a given case
in the data belongs to a group. In the case of binary classification,
the value falls between 0 and 1. There are many techniques for
determining the cut off threshold for classification; a typical method
is to assign anything above a 0.5 into the ``1'' or ``positive'' class.
An ROC curve (receiver operating characteristic curve) plots the True
Positive Rate vs.~the False Positive rate at different classification
thresholds; it is a measurement of the performance of a classification
model across all possible thresholds, and therefore sidesteps the need
to arbitrarily assign a cutoff.

AUC measures the entire two-dimensional area underneath the ROC curve.
It is the integration of the curve from (0,0) to (1,1), defined as
\(AUC = \int_{(0,0)}^{(1,1)} f(x)dx\). AUC enumerates the probability
that the model ranks a random positive example more highly than a random
negative example. A value of 0.5 represents a perfectly random model,
while a value of 1.0 represents a model that can perfectly discriminate
between the two classes. AUC is useful for comparing classification
models against one another because they are both scale and
threshold-invariant.

One of the drawbacks to AUC is that it does not describe the trade-offs
between false positives and false negatives. In certain circumstances, a
false positive might be considerably less desirable than a false
negative, or vice-versa. For our purposes, we rank false positives and
false negatives as equally undesirable outcomes.

\hypertarget{root-mean-squared-error}{%
\subsubsection{Root Mean Squared Error}\label{root-mean-squared-error}}

RMSE is a common measurement of the differences between values predicted
by a regression model and the observed values. It is formally defined as
\(RMSE = \sqrt{ \frac{\sum_{1}^{T} (\hat{y}_t - y_t)^2}{T} }\), where
\(\hat{y}\) represents the prediction and \(y\) represents the observed
value at observation \(t\).

Lower RMSE scores are typically more desirable. An RMSE value of 0 would
indicate a perfect fit to the data. RMSE can be difficult to interpret
on its own; however, it is useful for comparing models with similar
outcome variables. In our case, the outcome variables (Sales Price per
Square Foot) are consistent across modeling datasets, and therefore can
be reasonably compared using RMSE.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{summary-of-results}{%
\subsection{Summary of Results}\label{summary-of-results}}

We have conducted comparative analyses across a two-stage modeling
process. In Stage 1, using the Random Forest algorithm, we tested 3
competing feature engineering techniques (base, zip-code aggregation,
and spatial-lag aggregation) for both a classification task (predicting
the occurrence of a building sale) and a regression task (predicting the
sale price of a building). We analyzed the results of the first stage to
identify which geographies and building types our model assumptions
worked best. In Stage 2, using a subset of the modeling data (selected
via an analysis of the output from Stage 1), we compared four algorithms
-- GLM, Random Forest, GBM and ANN -- across our 3 competing feature
engineering techniques for both classification and regression tasks. We
analyzed the performance of the different model/data combos as well as
conducted an analysis of the variable importances for the top performing
models.

In Stage 1 (Random Forest, using all data), we found that models which
utilized spatial features outperformed those models using zip-code
features the majority of the time for both classification and
regression. Of three models, the Sale Price Regression model using
Spatial features finished 1st or 2nd 24.1\% of the time (using RMSE as a
ranking criterion), while the Zip Code Regression model finished in the
top two spots only 11.2\% of the time. Both models performed worse than
the Base Regression model overall, which ranked in 1st or 2nd place
31.5\% of the time. The story for the classification models was largely
the same: the Spatial features tended to outperform the Zip Code data
while the Base data won out overall. All models had similar performances
on training data, but the Spatial and Zip Code datasets tended to
underperform when generalizing to the hold-out test data, suggesting
problems with overfitting.

We then analyzed the performance of both the regression and
classification Random Forest models by geography and building type. We
found that the models performed considerably better on Walk Up
Apartments and Elevator Buildings (building types C and D) and in
Manhattan, Brooklyn and the Bronx. Using these as filtering criteria, we
created a subset of the modeling data for the subsequent modeling stage.

During Stage 2 (many algorithms using a subset of modeling data), we
compared four algorithms across the same three competing feature
engineering techniques using a filtered subset of the original modeling
data. Unequivocally, the spatial features performed best across all
models and tasks. For the classification task, the GBM algorithms
performed best in terms of AUC, followed by ANN and Random Forest. For
regression, the ANN algorithms performed best (as measured by RMSE as
well as Mean Absolute Error and R-squared) with the spatial features ANN
model performing best.

We conclude that spatial lag features can significantly increase the
accuracy of machine learning-based real estate sale prediction models.
We find that model overfitting presents a challenge when using spatial
features, but that this can be overcome by implementing different
algorithms, specifically ANN and GBM. Finally, we find that our
implementation of spatial-lag features works best for certain kinds of
buildings in specific geographic areas, and we hypothesize that this is
due to the assumptions made when building the spatial features.

\hypertarget{stage-1-random-forest-models-using-all-data}{%
\subsection{Stage 1) Random Forest Models Using All
Data}\label{stage-1-random-forest-models-using-all-data}}

\hypertarget{sale-price-regression-models}{%
\subsubsection{Sale Price Regression
Models}\label{sale-price-regression-models}}

We analyzed the RMSE of the Random Forest models predicting Sale Price
across feature engineering methods, Borough and Building Type. Table
\ref{tab:SalePriceModelRank} displays the average ranking by model type
as well as the distribution of models that ranked first, second and
third for each respective Borough/Building Type combination. When we
rank the models by performance for each Borough, Building Type
combination, we find that the Spatial Lag models outperform the Zip Code
models in 72\% of cases with an average model-rank of 2.11 and 2.5,
respectively.

\begin{table}

\caption{\label{tab:Sale Price Model Rank Distributions}\label{tab:SalePriceModelRank} Sale Price Model Rankings, RMSE by Borough and Building Type}
\centering
\begin{tabular}[t]{llllr}
\toprule
Model Rank & 1 & 2 & 3 & Average Rank\\
\midrule
Base & 22.2\% & 9.3\% & 1.9\% & 1.39\\
Spatial Lag & 5.6\% & 18.5\% & 9.3\% & 2.11\\
Zip & 5.6\% & 5.6\% & 22.2\% & 2.50\\
\bottomrule
\end{tabular}
\end{table}

The Base modeling dataset tends to outperform both enriched datasets,
suggesting an issue with model overfitting in some areas. We see further
evidence of overfitting in Table \ref{tab:SalePriceEval} where, despite
similar performances on Validation data, the Zip and Spatial models have
higher validation-to-test-set spreads. Despite this, the Spatial Lag
features outperform all other models in specific locations, notably in
Manhattan as shown in Figure \ref{fig:RMSE by boro and build type}.

\begin{table}

\caption{\label{tab:Sale Price Evaluations}\label{tab:SalePriceEval} Sale Price Model RMSE For Validation and Test Hold-out Data}
\centering
\begin{tabular}[t]{lrrr}
\toprule
type & base & zip & spatial lag\\
\midrule
Validation & 280.63 & 297.97 & 286.23\\
Test & 287.83 & 300.60 & 297.92\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/RMSE by boro and build type} \caption{RMSE By Borough and Building Type}\label{fig:RMSE by boro and build type}
\end{figure}

Figure \ref{fig:RMSE by boro and build type} displays test RMSE by
model, faceted by Borough on the y-axis and Building Type on the x-axis
(See Table \ref{tab:categoryTable} and Table \ref{tab:by_class} for a
description of building type codes). We make the following observations
from Figure \ref{fig:RMSE by boro and build type}:

\begin{itemize}
\tightlist
\item
  The Spatial modeling data outperforms both Base and Zip Code in 6
  cases, notably for Type A buildings (One Family Dwellings) and Type L
  buildings (Lofts) in Manhattan as well as Type O Buildings (Office) in
  Queens
\item
  The ``residential'' building Types A (One-family Dwellings), B (Two
  Family Dwellings), C (Walk Up Apartments) and D (Elevator Apartments)
  have generally lower RMSE scores compared to the non-residential types
\item
  Spatial features perform best in Brooklyn, Bronx and Manhattan and for
  residential building types
\end{itemize}

\hypertarget{probability-of-sale-classification-models}{%
\subsubsection{Probability of Sale Classification
Models}\label{probability-of-sale-classification-models}}

Similar to the results of the Sale Price regression models we find the
Spatial models perform better on the hold-out test data compared to the
Zip Code data when using Area Under the ROC Curve (AUC) as an evaluation
metric, as shown in Table \ref{tab:ProbSaleModelAUC}. The Base Modeling
data continues to outperform the Spatial and Zip Code data overall.

\begin{table}

\caption{\label{tab:Prob Model AUC}\label{tab:ProbSaleModelAUC} Probability of Sale Model AUC}
\centering
\begin{tabular}[t]{lrrr}
\toprule
Model AUC & Base & Zip & Spatial Lag\\
\midrule
Validation & 0.832 & 0.829 & 0.829\\
Test & 0.830 & 0.825 & 0.828\\
\bottomrule
\end{tabular}
\end{table}

Figure \ref{fig:AUC by boro and build type} shows a breakdown of model
AUC faceted along the x-axis by Building Type and along the y-axis by
Borough. The coloring indicates by how much a model's AUC diverges from
the cell average, which is useful for spotting over performers. We make
the following observations about Figure
\ref{fig:AUC by boro and build type}:

\begin{itemize}
\tightlist
\item
  The Spatial models outperform all other models for Elevator Buildings
  (Type D) and Walk Up Apartments (Type C), particularly in Brooklyn,
  the Bronx, and Manhattan
\item
  Classification tends to perform poorly in Manhattan vs.~other Boroughs
\item
  The Spatial models perform well in Manhattan for the residential
  building types (A, B, C and D)
\end{itemize}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/AUC by boro and build type} \caption{AUC By Borough and Building Type}\label{fig:AUC by boro and build type}
\end{figure}

If we rank the classification models' performance for each Borough and
Building Type, we see that the Spatial models consistently outperform
the Zip Code models, as shown in Table \ref{tab:ProbModelAUCRank}. From
this (as well as from similar patterns seen in the regression models) we
can infer that the Spatial data is a superior data engineering
technique; however, the algorithm used needs to account for potential
model overfitting. In the next section, we discuss refining the data
used as well as employing different algorithms to maximize the
predictive capability of the Spatial features.

\begin{table}

\caption{\label{tab:Prob Model AUC Average Rank}\label{tab:ProbModelAUCRank} Distribution and Average Model Rank for Probability of Sale by AUC across Borough and Building Types}
\centering
\begin{tabular}[t]{llllr}
\toprule
Model Rank & 1 & 2 & 3 & Average Rank\\
\midrule
Base & 16.2\% & 12.0\% & 5.1\% & 2.22\\
Spatial Lag & 11.1\% & 13.7\% & 8.5\% & 2.09\\
Zip & 6.0\% & 7.7\% & 19.7\% & 1.69\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{stage-2-model-comparisons-using-specific-geographies-and-building-types}{%
\subsection{Stage 2) Model Comparisons Using Specific Geographies and
Building
Types}\label{stage-2-model-comparisons-using-specific-geographies-and-building-types}}

Using the results from the first modeling exercise, we conclude that
Walk Up Apartments and Elevator Buildings in Manhattan, Brooklyn and the
Bronx are suitable candidates for prediction using our current
assumptions. These buildings share the characteristics of being
residential as well as being reasonably uniform in their geographic
density. We analyze the performance of four algorithms (GLM, RF, GBM,
and ANN), using three feature engineering techniques, for both
classification and regression, making the total number
\texttt{4\ x\ 3\ x\ 2\ =\ 24} models.

\hypertarget{regression-model-comparisons}{%
\subsubsection{Regression Model
Comparisons}\label{regression-model-comparisons}}

The predictive accuracies of the various regression models were
evaluated using RMSE, described in detail in the methodology section, as
well as Mean Absolute Error (MAE), Mean Squared Error (MSE) and
R-Squared. These four indicators were calculated using the hold-out test
data, which ensured that the models performed well when predicting sale
prices into the near future. The comparison metrics are presented in
Table \ref{tab:RegModelTable} and Figure
\ref{fig:Model RMSE Comparrison}. We make the following observations
about Table \ref{tab:RegModelTable} and Figure
\ref{fig:Model RMSE Comparrison}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The ANN models perform best in nearly every metric across nearly all
  feature sets, with GBM a close second in some circumstances
\item
  ANN and GLM improve linearly in all metrics as you move from Base to
  Zip to Spatial, with Spatial performing the best. GBM and Random
  Forest, on the other hand, perform best on the Base and Spatial
  feature sets and poorly on the Zip features
\item
  We see a similar pattern in the Random Forest results compared to the
  previous modeling exercise using the full dataset: Base Features
  outperform both Spatial and Zip, with Spatial coming in second
  consistently. This pattern further validates our reasoning that
  Spatial features are highly predictive but suffer from overfitting and
  other algorithm-related reasons
\item
  The highest model R-Squared is the ANN using Spatial features at
  0.494, indicating that this model can account for nearly 50\% of the
  variance in the test data. Compared to the R-Squared of the more
  traditional Base GLM at 0.12, this represents a more than 3-fold
  improvement in predictive accuracy
\end{enumerate}

\begin{table}

\caption{\label{tab:Reg Model RMSE Compare}\label{tab:RegModelTable} Prediction Accuracy of Regression Models on Test Data}
\centering
\begin{tabular}[t]{llrrrr}
\toprule
Data & Model & RMSE & MAE & MSE & R2\\
\midrule
1) Base & GLM & 446.35 & 221.16 & 199227.6 & 0.12\\
2) Zip & GLM & 426.93 & 206.49 & 182270.1 & 0.19\\
3) Spatial & GLM & 382.32 & 195.00 & 146170.5 & 0.35\\
1) Base & RF & 387.99 & 174.24 & 150536.3 & 0.33\\
2) Zip & RF & 475.20 & 190.33 & 225811.7 & 0.00\\
\addlinespace
3) Spatial & RF & 430.92 & 180.17 & 185695.5 & 0.18\\
1) Base & GBM & 384.11 & 179.27 & 147543.5 & 0.35\\
2) Zip & GBM & 454.53 & 186.00 & 206593.1 & 0.09\\
3) Spatial & GBM & 406.70 & 170.97 & 165408.0 & 0.27\\
1) Base & ANN & 363.02 & 178.58 & 131782.5 & 0.42\\
\addlinespace
2) Zip & ANN & 360.88 & 171.22 & 130232.2 & 0.42\\
3) Spatial & ANN & 337.94 & 158.91 & 114202.0 & 0.49\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/compare_reg_model_rmse} \caption{Comparative Regression Metrics}\label{fig:Model RMSE Comparrison}
\end{figure}

\noindent Figure \ref{fig:reg Models Scatterplot} shows clusters of
performance across R-Squared and Mean Absolute Error, with the ANN
models outperforming their peers. This figure also makes clear that the
marriage of Spatial features with the ANN algorithm results in a
dramatic reduction in error rate compared to the other techniques.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/reg_model_results_scatterplot} \caption{Regression Model Performances On Test Data}\label{fig:reg Models Scatterplot}
\end{figure}

\hypertarget{classification-model-comparisons}{%
\subsubsection{Classification Model
Comparisons}\label{classification-model-comparisons}}

The classification models were assessed using AUC as well as MSE, RMSE,
and R-squared. As with the regression models, these four metrics were
calculated using the hold-out test data, ensuring that the models
generalize well into the near future. The comparison metrics are
presented in Table \ref{tab:ClassModelTable}. Figure
\ref{fig:Model AUC Comparrison} shows the ROC curves and corresponding
AUC for each algorithm/feature set combination.

\noindent We make the following observations of Table
\ref{tab:ClassModelTable} and Figure \ref{fig:Model AUC Comparrison}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Unlike the regression models, the GBM algorithm with Spatial features
  proved to be the best performing classifier. All Spatial models
  performed relatively well except the GLM Spatial model
\item
  Only 3 models have positive R-Squared values: ANN Spatial, RF Spatial,
  and GLM Base, indicating that these models are adept at predicting
  positive cases (occurrences of sales) in the test data
\item
  GLM Spatial returned an AUC of less than 0.5, indicating a model that
  is conceptually worse than random. This is likely the result of
  overfitting
\end{enumerate}

\begin{table}

\caption{\label{tab:Class Model Compare}\label{tab:ClassModelTable} Prediction Accuracy of Classification Models on Test Data}
\centering
\begin{tabular}[t]{llrrrr}
\toprule
Data & Model & AUC & MSE & RMSE & R2\\
\midrule
1) Base & GLM & 0.57 & 0.03 & 0.17 & 0.00\\
2) Zip & GLM & 0.58 & 0.03 & 0.17 & 0.00\\
3) Spatial & GLM & 0.50 & 0.03 & 0.17 & -0.01\\
1) Base & RF & 0.58 & 0.03 & 0.17 & -0.03\\
2) Zip & RF & 0.56 & 0.03 & 0.17 & -0.06\\
\addlinespace
3) Spatial & RF & 0.78 & 0.03 & 0.17 & 0.00\\
1) Base & GBM & 0.61 & 0.03 & 0.17 & -0.03\\
2) Zip & GBM & 0.61 & 0.03 & 0.17 & -0.03\\
3) Spatial & GBM & 0.82 & 0.03 & 0.16 & 0.04\\
1) Base & ANN & 0.55 & 0.03 & 0.17 & -0.03\\
\addlinespace
2) Zip & ANN & 0.57 & 0.03 & 0.17 & -0.04\\
3) Spatial & ANN & 0.76 & 0.03 & 0.17 & -0.01\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/compare_model_roc_curves} \caption{Comparison of Classification Model ROC Curves}\label{fig:Model AUC Comparrison}
\end{figure}

\noindent Figure \ref{fig:Class Models Scatterplot} plots the individual
models by AUC and R-Squared. The Spatial models tend to outperform the
other models by a significant margin. Interestingly, when compared to
the regression model scatterplot in Figure
\ref{fig:reg Models Scatterplot}, the classification models tend to
cluster in their performance by feature set. In
\ref{fig:reg Models Scatterplot}, we see the regression models tending
to cluster by algorithm rather than features.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{Sections/tables and figures/class_model_results_scatterplot} \caption{Scatterplot of Classification Models}\label{fig:Class Models Scatterplot}
\end{figure}

\hypertarget{variable-importance-analysis-of-top-performing-models}{%
\subsection{Variable Importance Analysis of Top Performing
Models}\label{variable-importance-analysis-of-top-performing-models}}

We calculated feature importance for each variable as the proportional
to the average decrease in the squared error after including that
variable in the model. The most important variable gets a score of 1;
scores for other variables are derived by standardizing their measured
reduction in error relative to the largest one. The top 10 variables for
both the most successful regression and most successful classification
models are presented in tables \ref{tab:RegVarImp} and
\ref{tab:ClassVarImp}.

\begin{table}

\caption{\label{tab:Reg VarImp}\label{tab:RegVarImp} Feature Importance of Top Performing Regression Model}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llrl}
\toprule
Variable & Description & Scaled Importance (Max = 1) & Cumulative \%\\
\midrule
BuiltFAR & Floor area ratio built & 1.000 & 1.80\%\\
FacilFAR & Maximum Allowable Floor Area Ratio & 0.922 & 3.40\%\\
Last\_Sale\_Price\_Total & The previous sale price & 0.901 & 5.10\%\\
Last\_Sale\_Date & Date of last sale & 0.893 & 6.70\%\\
Last\_Sale\_Price & The previous sale price & 0.870 & 8.20\%\\
\addlinespace
Years\_Since\_Last\_Sale & Number of years since last sale & 0.823 & 9.70\%\\
ResidFAR & Floor Area Ratio not yet built & 0.814 & 11.20\%\\
lon & Longitude & 0.773 & 12.60\%\\
Year & Year of record & 0.759 & 13.90\%\\
BldgDepth & Square feet from font to back & 0.758 & 15.30\%\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}

\caption{\label{tab:Class VarImp}\label{tab:ClassVarImp} Feature Importance of Top Performing Classification Model}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llrl}
\toprule
Variable & Description & Scaled Importance (Max = 1) & Cumulative \%\\
\midrule
Percent\_Neighbords\_Sold & Percent of Nearby Properties Sold in the Previous Year & 1.000 & 21.90\%\\
Percent\_Office & Percent of the build which is Office & 0.698 & 37.20\%\\
Percent\_Garage & Percent of the build which is Garage & 0.634 & 51.10\%\\
Percent\_Storage & Percent of the build which is Storage & 0.518 & 62.40\%\\
Building\_Age & The Age of the building & 0.225 & 67.40\%\\
\addlinespace
Last\_Sale\_Price & Price of building last time is was sold & 0.165 & 71.00\%\\
Percent\_Retail & Percent of the build which is Retail & 0.147 & 74.20\%\\
Years\_Since\_Last\_Sale & Year since building last sold & 0.121 & 76.90\%\\
ExemptTot & Total tax exempted value of the building & 0.069 & 78.40\%\\
Radius\_Res\_Units\_Sold\_In\_Year & Residential units within 500 meters sold in past year & 0.056 & 79.60\%\\
\bottomrule
\end{tabular}}
\end{table}

We observe that the regression model has a much higher dispersion of
feature importances compared to the classification model. The top
variable in the regression model, BuiltFAR, which is a measure of how
much of a building's floor to area ratio has been used (a proxy for
overall building size) contributes only 1.8\% of the reduction in the
error rate in the overall model. Conversely, in the classification
model, we see the top variable, ``Percent\_Neighbors\_Sold'' (a measure
of how many buildings within 500 meters were sold in the past year)
contributes 21.9\% of the total reduction in squared error.

Variable importance analysis of the regression model indicates that the
model favors variables which reflect building size (BuiltFAR, FacilFAR,
BldgDepth) as well as approximations for previous sale prices
(Last\_Sale\_Price and Last\_Sale\_Date). The classification model tends
to favor spatial lag features, such as how many buildings were sold in
the past year within 500 meters (Percent\_Neighbors\_Sold and
Radius\_Res\_Units\_Sold\_In\_Year) as well as characteristics of the
building function (Percent\_Office, Percent\_Storage, etc.).

\hypertarget{future-research-and-conclusions}{%
\section{Future Research and
Conclusions}\label{future-research-and-conclusions}}

\hypertarget{future-research}{%
\subsection{Future Research}\label{future-research}}

This research has shown that the addition of spatial lag features can
meangingfully increase the predictive accuracy of machine learning
models compared to traditional real estate valuation techniques. There
are several areas that could be further explored regarding
spatially-conscious machine learning models, some of which we mention
below.

First, it became apparent in the research that generalization was a
problem for some of the models, likely due to overfitting of the
training data. We corrected for this issue by employing more robust
algorithms; however, further work could be done to create variable
selection processes or hyperparameter tuning to prevent overfit and
generally accelerate model training and scoring.

Additionally, the spatial lag features seemed to perform best for
certain boroughs and residential building types. We hypothesize that
using a 500-meter radius to build spatial lag features, a distance which
we arbitrarily chose, works best for this type of asset in these areas.
Fotheringham (2015) used an ``Adaptive Bandwidth'' technique to adjust
the spatial lag radius based on cross-validation with much success. The
techniques presented in this paper could be expanded to use
cross-validation similarly to assign the optimal spatial lag radius for
each building type and location. Furthermore, additional work could be
done using cross-validation or a similar technique to refine the spatial
lags further to include appropriate building types and distance weights
tailored to each property.

Finally, this research aimed to predict real estate transactions 1 year
into the future. While this is a promising start, 1-year of lead time
may not be sufficient to respond to growing gentrification challenges.
In addition, modeling at the annual level could be improved to quarterly
or monthly, given that the sales data contains date information down to
the day. To make this system practical for combating displacement, it
may be helpful to predict at a more granular level and further into the
future.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

Societies and communities can benefit materially from gentrification,
however, the downside should not be overlooked. Displacement causes
Economic Exclusion, which over time contributes to rising income
inequality. Combating displacement allows communities to benefit from
gentrification without suffering the negative consequences. One way to
practically combat displacement is to predict gentrification, which this
paper has attempted to do.

Spatial lags, typically seen in geographically weighted regression, were
employed successfully to enhance the predictive power of machine
learning models. The spatial lag models performed best for particular
building types and geographies; however, we feel confident that the
technique could be expanded to work equally as well for all buildings
with some additional research. Regarding algorithms, Artificial Neural
Networks performed the best for predicting sale price, while Gradient
Boosting Machines performed best for predicting sale occurrence.

While this research is not intended to serve as a full early-warning
system for gentrification and displacement, it is a step in that
direction. More research is needed to help address the challenges faced
by city planners and governments trying to help incumbent residents reap
the benefits of local investments. Income inequality is a complicated
and grave issue, but new tools and techniques to inform and prevent will
help ensure equality of opportunity for all.

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Almanie2015}{}%
Almanie, R.; Lor, T.; Mirza. 2015. ``Crime Prediction Based on Crime
Types and Using Spatial and Temporal Criminal Hotspots.''
\emph{International Journal of Data Mining \& Knowledge Management
Process (IJDKP)} 5 (4).

\leavevmode\hypertarget{ref-antipov12}{}%
Antipov, Evgeny A., and Elena B. Pokryshevskaya. 2012. ``Mass Appraisal
of Residential Apartments: An Application of Random Forest for Valuation
and a Cart-Based Approach for Model Diagnostics.'' \emph{Expert Systems
with Applications}.

\leavevmode\hypertarget{ref-Pollack2010}{}%
Barry Bluestone \& Chase Billingham, Stephanie Pollack \&. 2010.
``Maintaining Diversity in America's Transit-Rich Neighborhoods: Tools
for Equitable Neighborhood Change.'' \emph{New England Community
Developments, Federal Reserve Bank of Boston}, 1--6.

\leavevmode\hypertarget{ref-Batty2013}{}%
Batty, Michael. 2013. ``The New Science of Cities.'' \emph{MIT Press}.

\leavevmode\hypertarget{ref-breiman2001random}{}%
Breiman, Leo. 2001. ``Random Forests.'' \emph{Machine Learning} 45 (1).
Springer: 5--32.

\leavevmode\hypertarget{ref-Chapple2009}{}%
Chapple, Karen. 2009. ``Mapping Susceptibility to Gentrification: The
Early Warning Toolkit.'' \emph{Berkeley, CA: Center for Community
Innovation.}

\leavevmode\hypertarget{ref-Chapple2016}{}%
Chapple, Miriam, Karen; Zuk. 2016. ``Forewarned: The Use of Neighborhood
Early Warning Systems for Gentrification and Displacement.''
\emph{Cityscape: A Journal of Policy Development and Research} 18 (3).

\leavevmode\hypertarget{ref-Clay1979}{}%
Clay, Phillip L. 1979. \emph{Neighborhood Renewal: Middle-Class
Resettlement and Incumbent Upgrading in American Neighborhoods}.
Lexington Books.

\leavevmode\hypertarget{ref-Springer2017}{}%
d'Amato, Tom, Maurizio; Kauko, ed. 2017. \emph{Advances in Automated
Valuation Modeling}. Springer International Publishing.

\leavevmode\hypertarget{ref-Dietzell2014}{}%
Dietzell, Nicole; Schfers, Marian Alexander; Braun. 2014.
``Sentiment-Based Commercial Real Estate Forecasting with Google Search
Volume Data.'' \emph{Journal of Property Investment \& Finance,} 32 (6):
540--69.

\leavevmode\hypertarget{ref-Dreier2004}{}%
Dreier, John; Swanstrom, Peter; Mollenkopf. 2004. \emph{Place Matters:
Metropolitics for the Twenty-First Century.} University Press of Kansas.

\leavevmode\hypertarget{ref-Eckert1990}{}%
Eckert, J. K. 1990. \emph{Property Appraisal and Assessment
Administration}. Chicago, IL.: International Association of Assessing
Officers.

\leavevmode\hypertarget{ref-Fotheringham2015}{}%
Fotheringham, R; Yao, A.S.; Crespo. 2015. ``Exploring, Modelling and
Predicting Spatiotemporal Variations in House Prices.'' \emph{The Annals
of Regional Science} 54.

\leavevmode\hypertarget{ref-friedman2002stochastic}{}%
Friedman, Jerome H. 2002. ``Stochastic Gradient Boosting.''
\emph{Computational Statistics \& Data Analysis} 38 (4). Elsevier:
367--78.

\leavevmode\hypertarget{ref-Fu2014}{}%
Fu, Yanjie; et al. 2014. \emph{Exploiting Geographic Dependencies for
Real Estate Appraisal: A Mutual Perspective of Ranking and Clustering}.
Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery; data mining.

\leavevmode\hypertarget{ref-Geltner2017}{}%
Geltner, David, and Alex Van de Minne. 2017. ``Do Different Price Points
Exhibit Different Investment Risk and Return Commercial Real Estate.''
Real Estate Research Institute.

\leavevmode\hypertarget{ref-Guan2014}{}%
Guan, Jian, Donghui Shi, Jozef M. Zurada, and Alan S. Levitan. 2014.
``Analyzing Massive Data Sets: An Adaptive Fuzzy Neural Approach for
Prediction, with a Real Estate Illustration.'' \emph{Journal of
Organizational Computing and Electronic Commerce} 24 (1). Taylor \&
Francis: 94--112. \url{https://doi.org/10.1080/10919392.2014.866505}.

\leavevmode\hypertarget{ref-hastie01statisticallearning}{}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. \emph{The
Elements of Statistical Learning}. Springer Series in Statistics. New
York, NY, USA: Springer New York Inc.

\leavevmode\hypertarget{ref-Helbich2013}{}%
Helbich, et al., Marco. 2013. ``Boosting the Predictive Accuracy of
Urban Hedonic House Price Models Through Airborne Laser Scanning.''
\emph{Computers, Environment and Urban Systems} 39: 81--92.

\leavevmode\hypertarget{ref-hoffmann2004generalized}{}%
Hoffmann, John Patrick. 2004. \emph{Generalized Linear Models: An
Applied Approach}. Pearson College Division.

\leavevmode\hypertarget{ref-Huang1996}{}%
Huang, Chong-Wei. 1996. ``On the Complexity of Point-in-Polygon
Algorithms.'' \emph{Computers and Geosciences} 23.

\leavevmode\hypertarget{ref-Johnson2007}{}%
Johnson, Ken, Justin Benefield, and Jonathan Wiley. 2007. ``The
Probability of Sale for Residential Real Estate.'' \emph{Journal of
Housing Research} 16 (2): 131--42.
\url{https://doi.org/10.5555/jhor.16.2.0234g75800h5k8x6}.

\leavevmode\hypertarget{ref-Kontrimasa2011}{}%
Kontrimasa, Antanas, Vilius; Verikasb. 2011. ``The Mass Appraisal of the
Real Estate by Computational Intelligence.'' \emph{Applied Soft
Computing}.

\leavevmode\hypertarget{ref-Koschinsky2012}{}%
Koschinsky, J. et al. 2012. ``The Welfare Benefit of a Home's Location:
An Empirical Comparison of Spatial and Non-Spatial Model Estimates.''
\emph{Journal of Geographical Systems} 10109.

\leavevmode\hypertarget{ref-Lees2008}{}%
Lees, Tom; Wyly, Loretta; Slater. 2008. ``Gentrification.'' \emph{Growth
and Change} 39 (3): 536--39.
\url{https://doi.org/10.1111/j.1468-2257.2008.00443.x}.

\leavevmode\hypertarget{ref-Miller2015}{}%
Miller, J.; Aspinall, J.; Franklin. 2007. ``Incorporating Spatial
Dependence in Predictive Vegetation Models.'' \emph{Ecological
Modelling} 202 (3): 225--42.

\leavevmode\hypertarget{ref-Park2015}{}%
Park, Jae Kwon, Byeonghwa; Bae. 2015. ``Using Machine Learning
Algorithms for Housing Price Prediction: The Case of Fairfax County,
Virginia Housing Data.'' \emph{Expert Systems with Applications} 42 (6):
2928--34.

\leavevmode\hypertarget{ref-Pivo2011}{}%
Pivo, Gary, and Jeffrey D. Fisher. 2011. ``The Walkability Premium in
Commercial Real Estate Investments.'' \emph{Real Estate Economics} 39
(2): 185--219. \url{https://doi.org/10.1111/j.1540-6229.2010.00296.x}.

\leavevmode\hypertarget{ref-Quintos2013}{}%
Quintos, Carmela. 2013. ``Estimating Latent Effects in Commercial
Property Models.'' \emph{Journal of Property Tax Assessment \&
Administration} 12 (2).

\leavevmode\hypertarget{ref-Rafiei2016}{}%
Rafiei, Hojjat, Mohammad Hossein; Adeli. 2016. ``A Novel Machine
Learning Model for Estimation of Sale Prices of Real Estate Units.''
\emph{Journal of Construction Engineering and Management} 142 (2).

\leavevmode\hypertarget{ref-Reardon2011}{}%
Reardon, Kendra, Sean F.; Bischoff. 2011. ``Income Inequality and Income
Segregation.'' \emph{American Journal of Sociology}.

\leavevmode\hypertarget{ref-Ritter2013}{}%
Ritter, Nancy. 2013. ``Predicting Recidivism Risk: New Tool in
Philadelphia Shows Great Promise.'' \emph{National Institute of Justice
Journal} 271.

\leavevmode\hypertarget{ref-Schernthanner2016}{}%
Schernthanner H., Gonschorek J., Asche H. 2016. ``Spatial Modeling and
Geovisualization of Rental Prices for Real Estate Portals.''
\emph{Computational Science and Its Applications} 9788.

\leavevmode\hypertarget{ref-SCHMIDHUBER201585}{}%
Schmidhuber, Jrgen. 2015. ``Deep Learning in Neural Networks: An
Overview.'' \emph{Neural Networks} 61: 85--117.
\url{https://doi.org/https://doi.org/10.1016/j.neunet.2014.09.003}.

\leavevmode\hypertarget{ref-Silverherz1936}{}%
Silverherz, J. D. 1936. ``The Assessment of Real Property in the United
States.'' \emph{Albany: J.B. Lyon Co. Printers}.

\leavevmode\hypertarget{ref-Smith1979}{}%
Smith, Neil. 1979. ``Toward a Theory of Gentrification a Back to the
City Movement by Capital, Not People.'' \emph{Journal of the American
Planning Association} 45 (4). Routledge: 538--48.
\url{https://doi.org/10.1080/01944367908977002}.

\leavevmode\hypertarget{ref-urban2016}{}%
Solomon Greene, Molly Scott, Rolf Pendall, and Serena Lei. 2016. ``Open
Cities: From Economic Exclusion to Urban Inclusion.'' \emph{Urban
Institue Brief}. Urban Institue Brief.

\leavevmode\hypertarget{ref-Turner2001}{}%
Turner, Margery Austin, and Christopher Snow. 2001. \emph{Leading
Indicators of Gentrification in D.c. Neighborhoods}.

\leavevmode\hypertarget{ref-Watson2009}{}%
Watson, Tara. 2009. ``Inequality and the Measurement of Residential
Segregation by Income in American Neighborhoods.'' \emph{Review of
Income and Wealth}.

\leavevmode\hypertarget{ref-Zuk2015}{}%
Zuk, Miriam; et al. 2015. ``Gentrification, Displacement and the Role of
Public Investment: A Literature Review.''


\end{document}
