---
title: "Results"
output: pdf_document
bibliography: bib/bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
suppressPackageStartupMessages({library(tidyverse)})
```


## Summary of Results

We have conducted comparative analyses across a two-stage modeling proces. In Stage 1, using the Random Forrest algorithm, we tested 3 competing feature engineering techniques (base, zip-code aggregation and spatial-lag aggreation) for both a classification task (predicting the occurance of a building sale) and a regression task (predicting the sale price of a building). We analyzed the results of the first stage to identify which geographies and building types our model assumptions worked best. In Stage 2, using a subset of the modeling data (selected via an analysis of the output from Stage 1), we compared four algorithms--Generalized Linear Model (GLM), Random Forrest (RF), Gradient Boosted Machine (GBM) and Feed-Forward Multilayer Artificial Neural Network (ANN)--across our 3 competing feature engineering techniques for both classification and regression tasks. We analyzed the performance of the different model/data combos as well as conducted an analysis of the variable importances for the top performing models. 

In Stage 1 (Random Forrest, using all data), we found that models which utilized spatial features outperformed those models using zip-code features the majority of the time for both classification and regression. Of three models, the Sale Price Regression model using Spatial features finished 1st or 2nd 24.1% of the time (using Root Mean Squared Error as a ranking criterion), while the Zip Code Regression model finished in the top two spots only 11.2% of the time. Both models performed worse than the Base Regression model overall, which ranked in 1st or 2nd place 31.5% of the time. The story for the classification models was largely the same: the Spatial features tended to outperform the Zip Code data while the Base data won out overall. All models had similar performances on training data, but the Spatial and Zip Code datasets tended to underperform when generalizing to the hold-out test data, suggesting problems with overfitting.

We then analyzed the performance of both the regression and classification Random Forrest models by geography and building type. We found that the models performed considerably better on Walk Up Apartments and Elevator Buildings (building types C and D) and in Manhattan, Brooklyn and the Bronx. Using these as filtering criteria, we created a subset of the modeling data for the subsequent modeling stage. 

During Stage 2 (many algorithms using a subset of modeling data), we compared four algorithms across the same three competing feature engineering techniques using a filtered subset of the original modeling data. Unequivocally, the spatial features performed best across all models and tasks. For the classification task, the GBM algorithms performed best in terms of AUC, followed by ANN and Random Forrest. For regression, the ANN algorithms performed best (as measured by Mean Absolute Error, Root Mean Squared Error and R-squared) with the spatial features ANN model performing best. 

We conclude that spatial lag features can significantly increase the accuracy of machine learning-based real estate sale prediction models. We find that model overfitting presents a challenge when using spatial features, but that this can be overcome by implementing different algorithms, specifically ANN and GBM. Finally, we find that our implementation of spatial-lag features works best for certain kinds of buildings in specific geographic areas, and we hypothesize that this is due to the assumptions made when building the spatial features. 

## Stage 1) Random Forrest Models Using All Data

### Results of Random Forrest Sale Pice Regression Model

We analyzed the the Root Mean Squared Errors (RMSE) of the Random Forrest models predicting Sale Price across feature engineering methods, Borough and Building Type. Table \ref{tab:SalePriceModelRank} displays the average ranking by model type as well as the distribution of models that ranked first, second and third for each respective Borough/Building Type combination. When we rank the models by performance for each Borough, Building Type combination, we find that the Spatial Lag models outperform the Zip Code models in 72% of cases with an average model-rank of 2.11 and 2.5, respectively. 

```{r Sale Price Model Rank Distributions, fig.cap= "Sale Price Model Evaluations", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/model rank perc distribution table.csv"
         , check.names = FALSE) %>% 
  left_join(read_rds("tables and figures/average sales model rank rmse.rds")
            , by = c("Model Rank" = "Model")
            , sort = F) %>% 
  my_kable(caption = "Sale Price Model Rankings, RMSE by Borough and Building Type"
           , label = "SalePriceModelRank"
           , latex_options = "basic")


```


The Base modeling data set tends to outperform both enriched datasets, suggesting an issue with model overfitting in some areas. We see further evidence of overfit in Table \ref{tab:SalePriceEval} where, despite similar performances on Validation data, the Zip and Spatial models have higher validation-to-test-set spreads. Despite this, the Spatial Lag features outperform all other models in certain locations, notably in Manhattan as shown in Figure \ref{fig:RMSE by boro and build type}. 


```{r Sale Price Evaluations, fig.cap= "Sale Price Model Evaluations", echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

read_rds("tables and figures/sales model evaluations.rds") %>% 
  rename("spatial lag" = radii) %>% 
  mutate_at(vars(base:`spatial lag`), funs(round),2) %>% 
  my_kable(caption = "Sale Price Model RMSE For Validation and Test Hold-out Data"
           , label = "SalePriceEval"
           , latex_options = "basic")
```


```{r RMSE by boro and build type, fig.cap="RMSE By Borough and Building Type", out.width = '100%', echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics("Sections/tables and figures/RMSE by boro and build type.jpeg")
```


Figure \ref{fig:RMSE by boro and build type} displays test RMSE by model, faceted by Borough on the y-axis and Building Type on the x-axis (See Table \ref{tab:categoryTable} and Table \ref{tab:by_class} for a description of building type codes). We make the following observations from Figure \ref{fig:RMSE by boro and build type}:

- The Spatial modeling data outperforms both Base and Zip Code in 6 cases, notably for Type A buildings (One Family Dwellings) and Type L buildings (Lofts) in Manhattan as well as Type O Buildings (Office) in Queens
- The "residential" building Types A (One Family Dwellings), B (Two Family Dwellings), C (Walk Up Apartments) and D (Elevator Apartments) have generally lower RMSE scores compared to the non-residential types
- Spatial features perform best in Brooklyn, Bronx and Manhattan and for residential building types


### Results of Random Forrest Probability of Sale Classification Model

Similar to the results of the Sale Price regression models we find the Spatial models perform better on the hold-out test data compared to the Zip Code data when using Area Under the ROC Curve (AUC) as an evaluation metric, as shown in Table  \ref{tab:ProbSaleModelAUC}. The Base Modeling data continues to outperform the Spatial and Zip Code data overall.

```{r Prob Model AUC, fig.cap= "Sale Price Model Evaluations", echo=FALSE, message=FALSE, warning=FALSE}
read.csv("tables and figures/Prob Models AUC evaluations.csv", check.names = FALSE) %>% 
  mutate_at(vars(Base:`Spatial Lag`), funs(round),3) %>% 
  my_kable(caption = "Probability of Sale Model AUC"
           , label = "ProbSaleModelAUC"
           , latex_options = "basic")
```

Figure \ref{fig:AUC by boro and build type} shows a breakdown of model AUC faceted along the x-axis by Building Type and along the y-axis by Borough. The coloring indicates by how much a model's AUC diverges from the cell average, which is useful for spotting overperformers. We make the following observations about Figure \ref{fig:AUC by boro and build type}:

- The Spatial models outperform all other models for Elevator Buildings (Type D) and Walk Up Apartments (Type C), particularly in Brooklyn, the Bronx and Manhattan
- Classification tends to perform poorly in Manhattan vs. other Boroughs
- The Spatial models performs well in Manhattan for the residential building types (A, B, C and D)


```{r AUC by boro and build type, echo=FALSE, fig.cap="AUC By Borough and Building Type", message=FALSE, warning=FALSE, out.width='100%', paged.print=FALSE}
knitr::include_graphics("Sections/tables and figures/AUC by boro and build type.jpeg")
```


If we rank the classification models' performance for each Borough and Building Type, we see that the Spatial models consistently outperform the Zip Code models, as shown in  Table \ref{tab:ProbModelAUCRank}. From this (as well as from similar patterns seen in the regression models) we can infer that the Spatial data is a superior data engineering technique, however, the algorithm used needs to account for potential model overfitting. In the next section, we discuss refining the data used as well as employing different algorithms to maximize predictive capability of the Spatial features.  

```{r Prob Model AUC Average Rank, fig.cap= "Sale Price Model Evaluations", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/auc model rank perc distribution table.csv"
         , check.names = FALSE) %>%
  left_join(read.csv("tables and figures/average prob model rank auc.csv"
                     , check.names = FALSE)
            , by = c("Model Rank" = "Model")
            ) %>% 
  my_kable(caption = "Distribution and Average Model Rank for Probability of Sale by AUC across Borough and Building Types"
           , latex_options = "basic"
           , label = "ProbModelAUCRank")

```


## Model Comparisons Using Specific Geographies and Building Types

Using the results from the first modeling exercise, we conclude that Walk Up Apartments and Elevator Buildings in Manhattan, Brooklyn and the Bronx are suitable candidates for prediction using our current assumptions. These buildings share the characteristics of being residential as well as being fairly uniform in their geographic density. We analyze the performance of four algorithms (GLM, RF, GBM and ANN), using three feature engineering techniques, for both classification and regression, making the total number `4 x 3 x 2 = 24` models.


### Regression Model Comparisons

The following criteria, described in detail in the Methodology section, were used to assess the predictive accuracy of the various regression models: Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Squared Error (MSE) and R-Squared. These four indicators are calculated using the hold-out test data, which allows us to ensure that the models perform well when predicting sale prices into the near future. The comparison metrics are presented in Table \ref{tab:RegModelTable} and Figure \ref{fig:Model RMSE Comparrison}.

\noindent The following conclusions can be made from Table \ref{tab:RegModelTable} and Figure \ref{fig:Model RMSE Comparrison}: 

1) The feed-forward Multilayer Artificial Neural Networks (ANN) perform best in nearly every metric across nearly all feature sets, with Gradient Boosted Machine (GBM) a close second in some circumstances
2) ANN and Generalized Linear Models (GLM) improve linearly in all metrics as you move from Base to Zip to Spatial, with Spatial performing the best. GBM and Random Forrest, on the other hand, perform best on the Base and Spatial feature sets and poorly on the Zip features 
3) The highest model R-Squared is the ANN using Spatial features at 0.494, indicating that this model can account for nearly 50% of the variance in the test data
4) We see a similar pattern in the Random Forrest results compared to the previous modeling exercise using the full dataset: Base Features outperforming both Spatial and Zip, with Spatial coming in second consistently. This further validates our reasoning that Spatial features are highly predictive but suffer from overfitting and other algorithm-related reasons


```{r Reg Model RMSE Compare, fig.cap= "Prediction Accuracy of Regression Models on Test Data", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/regression_models_summary.csv"
         , check.names = FALSE) %>%
  mutate_at(vars(RMSE:R2), funs(round),2) %>% 
  my_kable(caption = "Prediction Accuracy of Regression Models on Test Data"
             , latex_options = "basic"
             , label = "RegModelTable")
               
```



```{r Model RMSE Comparrison, echo=FALSE, fig.cap="Comparative Regression Metrics", message=FALSE, warning=FALSE, out.width='100%', paged.print=FALSE}

knitr::include_graphics("Sections/tables and figures/compare_reg_model_rmse.jpeg")

```

\noindent Figure \ref{fig:reg Models Scatterplot} shows clusters of performance across R-Squared and Mean Absolute Error, with the ANN models outperforming their peers. This figure also makes clear that the marriage of Spatial features with the ANN algorithm results in a dramatic reduction in error rate compared to the other techniques. 

```{r reg Models Scatterplot, echo=FALSE, fig.cap="Regression Model Performances On Test Data", message=FALSE, warning=FALSE, out.width='100%', paged.print=FALSE}
knitr::include_graphics("Sections/tables and figures/reg_model_results_scatterplot.jpeg")
```



### Classification Model Comparisons

The criteria used to assess the accuracy of the classification models are as follows: Area Under the ROC Curve (AUC), Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and R-squared (R2). As with the regression models, these four indicators are calculated using the hold-out test data, allowing us to ensure that the models generalize well into the near future. The comparison metrics are presented in Table \ref{tab:ClassModelTable}.  Figure \ref{fig:Model AUC Comparrison} shows the ROC curves and corrsponding AUC for each algorithm/feature set combination. 

\noindent The following conclusions can be made from Table \ref{tab:ClassModelTable} and Figure \ref{fig:Model AUC Comparrison}: 

1) Unlike the regression models, the GBM algorithm with Spatial features proved to be the best performing classifier. All Spatial models performed relatively well with the exception of the GLM Spatial model
2) The error metrics tend to be quite low and frequently negative given that they are calculated using probabilitites and classes (0,1)
3) Only 3 models have positive R-Squared values: ANN Spatial, RF Spatial and GLM Base. This would indicate that these models are adept at prediciting positive cases (occurances of sales) in the test data
4) GLM Spatial returned an AUC of less than 0.5, indicating a model that is conceptually worse than random. This is likely a result of extreme overfitting


```{r Class Model Compare, fig.cap= "Prediction Accuracy of Classification Models on Test Data", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/classification-results-table.csv"
         , check.names = FALSE) %>%
  mutate_at(vars(AUC:Gini), funs(round),2) %>%
  select(-Logloss,-Gini) %>% 
  my_kable(caption = "Prediction Accuracy of Classification Models on Test Data"
             , latex_options = "basic"
             , label = "ClassModelTable")
               
```



```{r Model AUC Comparrison, echo=FALSE, fig.cap="Comparison of Classification Model ROC Curves", message=FALSE, warning=FALSE, out.width='100%', paged.print=FALSE}

knitr::include_graphics("Sections/tables and figures/compare_model_roc_curves.jpeg")

```


\noindent Figure \ref{fig:Class Models Scatterplot} plots the individual models by AUC and R-Squared. The Spatial models tend to outperform the other models by a significant margin. Interestingly, when compared to the regression model scatterplot, Figure \ref{fig:reg Models Scatterplot}, the classification models tend to cluster in their performance by feature set. In \ref{fig:reg Models Scatterplot}, we see the regression models tending to cluster by algorithm rather than features. 


```{r Class Models Scatterplot, echo=FALSE, fig.cap="Scatterplot of Classification Models", message=FALSE, warning=FALSE, out.width='100%', paged.print=FALSE}
knitr::include_graphics("Sections/tables and figures/class_model_results_scatterplot.jpeg")
```


## Variable Importance Analysis of Top Performing Models

Feature importance is calculated for each algorthm as being proportional to the average decrease in the squared error after including that variable in the model. The most important variable gets a score of 1; scores for other variables are derived by standardizing their measured reduction in error relative to the largest one. The top 10 variables for both the most successful regression and most successful classification models are presented in tables \ref{tab:RegVarImp} and \ref{tab:ClassVarImp}. 


```{r Reg VarImp, fig.cap= "Feature Importance of Top Performing Regression Model", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/regression_feat_impo_w_descr.csv"
         , check.names = FALSE) %>%
  my_kable(caption = "Feature Importance of Top Performing Regression Model"
             , latex_options = "scale_down"
             , label = "RegVarImp")
               
```


```{r Class VarImp, fig.cap= "Feature Importance of Top Performing Classification Model", echo=FALSE, message=FALSE, warning=FALSE}

read.csv("tables and figures/classification_feat_impo_w_descr.csv"
         , check.names = FALSE) %>%
  my_kable(caption = "Feature Importance of Top Performing Classification Model"
             , latex_options = "scale_down"
             , label = "ClassVarImp")

```

We observe that the regression model has a much higher dispersion of feature importances compared to the classification model. The top variable in the regression model, BuiltFAR, which is a measure of how much of a building's floor to area ratio has been used (a proxy for overall building size) contributing only 1.8% of the reduction in error rate in the overall model. Conversely, in the classification model, we see the top variable, "Percent_Neighbors_Sold" (a measure of how many buildings within 500 meters were sold in the past year) contributes 21.9% of the total reduction in squared error. 

Variable importance analysis of the regression model indicates that the model favors variables which reflect building size (BuiltFAR, FacilFAR, BldgDepth) as well as approximations for previous sale prices (Last_Sale_Price and Last_Sale_Date). The classification model tends to favor spatial lag features, such as how many buildings were sold in the past year within 500 meters (Percent_Neighbors_Sold and Radius_Res_Units_Sold_In_Year) as well as characteristics of the building function (Percent_Office, Percent_Storage, etc.). 







